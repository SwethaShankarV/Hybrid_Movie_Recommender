{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/swethashankar/Documents/GitHub/MovieLens_DL/movielens/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2952159404.py:4: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata_df= pd.read_csv(\"/Users/swethashankar/Documents/GitHub/MovieLens_DL/archive/movies_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "credits_df= pd.read_csv(\"/Users/swethashankar/Documents/GitHub/MovieLens_DL/archive/credits.csv\")\n",
    "keywords_df= pd.read_csv(\"/Users/swethashankar/Documents/GitHub/MovieLens_DL/archive/keywords.csv\")\n",
    "links_df= pd.read_csv(\"/Users/swethashankar/Documents/GitHub/MovieLens_DL/archive/links_small.csv\")\n",
    "metadata_df= pd.read_csv(\"/Users/swethashankar/Documents/GitHub/MovieLens_DL/archive/movies_metadata.csv\")\n",
    "ratings_df= pd.read_csv(\"/Users/swethashankar/Documents/GitHub/MovieLens_DL/archive/ratings_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies: (45466, 24)\n",
      "Credits: (45476, 3)\n",
      "Keywords: (46419, 2)\n",
      "Ratings: (100004, 4)\n",
      "Links: (9125, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Movies:\", metadata_df.shape)\n",
    "print(\"Credits:\", credits_df.shape)\n",
    "print(\"Keywords:\", keywords_df.shape)\n",
    "print(\"Ratings:\", ratings_df.shape)\n",
    "print(\"Links:\", links_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Remove rows with null critical fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credits:\n",
      " cast    False\n",
      "crew    False\n",
      "id      False\n",
      "dtype: bool\n",
      "\n",
      "keywords:\n",
      " id          False\n",
      "keywords    False\n",
      "dtype: bool\n",
      "\n",
      "links small:\n",
      " movieId    False\n",
      "imdbId     False\n",
      "tmdbId      True\n",
      "dtype: bool\n",
      "\n",
      "movies metadata:\n",
      " adult                    False\n",
      "belongs_to_collection     True\n",
      "budget                   False\n",
      "genres                   False\n",
      "homepage                  True\n",
      "id                       False\n",
      "imdb_id                   True\n",
      "original_language         True\n",
      "original_title           False\n",
      "overview                  True\n",
      "popularity                True\n",
      "poster_path               True\n",
      "production_companies      True\n",
      "production_countries      True\n",
      "release_date              True\n",
      "revenue                   True\n",
      "runtime                   True\n",
      "spoken_languages          True\n",
      "status                    True\n",
      "tagline                   True\n",
      "title                     True\n",
      "video                     True\n",
      "vote_average              True\n",
      "vote_count                True\n",
      "dtype: bool\n",
      "\n",
      "ratings small:\n",
      " userId       False\n",
      "movieId      False\n",
      "rating       False\n",
      "timestamp    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(\"credits:\\n\", credits_df.isnull().any())\n",
    "print(\"\\nkeywords:\\n\", keywords_df.isnull().any())\n",
    "print(\"\\nlinks small:\\n\", links_df.isnull().any())\n",
    "print(\"\\nmovies metadata:\\n\", metadata_df.isnull().any())\n",
    "print(\"\\nratings small:\\n\", ratings_df.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links small:\n",
      " movieId     0\n",
      "imdbId      0\n",
      "tmdbId     13\n",
      "dtype: int64\n",
      "movies metadata:\n",
      " adult                        0\n",
      "belongs_to_collection    40972\n",
      "budget                       0\n",
      "genres                       0\n",
      "homepage                 37684\n",
      "id                           0\n",
      "imdb_id                     17\n",
      "original_language           11\n",
      "original_title               0\n",
      "overview                   954\n",
      "popularity                   5\n",
      "poster_path                386\n",
      "production_companies         3\n",
      "production_countries         3\n",
      "release_date                87\n",
      "revenue                      6\n",
      "runtime                    263\n",
      "spoken_languages             6\n",
      "status                      87\n",
      "tagline                  25054\n",
      "title                        6\n",
      "video                        6\n",
      "vote_average                 6\n",
      "vote_count                   6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"links small:\\n\", links_df.isnull().sum())\n",
    "print(\"movies metadata:\\n\", metadata_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_cleaned= links_df.dropna(subset=[\"tmdbId\"]).copy()\n",
    "critical_fields=[\"imdb_id\" , \"original_language\", \"overview\", \"popularity\",\"title\"]\n",
    "metadata_cleaned= metadata_df.dropna(subset=critical_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9112, 3)\n",
      "(44481, 24)\n"
     ]
    }
   ],
   "source": [
    "print(links_cleaned.shape)\n",
    "print(metadata_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Parse JSON columns into lists of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres with suspicious values:\n",
      "[]\n",
      "\n",
      "Keywords with suspicious values:\n",
      "[]\n",
      "\n",
      "Cast with suspicious values:\n",
      "[]\n",
      "\n",
      "Crew with suspicious values:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Genres with suspicious values:\")\n",
    "print(metadata_df[~metadata_df[\"genres\"].astype(str).str.startswith(\"[\")][\"genres\"].unique())\n",
    "\n",
    "print(\"\\nKeywords with suspicious values:\")\n",
    "print(keywords_df[~keywords_df[\"keywords\"].astype(str).str.startswith(\"[\")][\"keywords\"].unique())\n",
    "\n",
    "print(\"\\nCast with suspicious values:\")\n",
    "print(credits_df[~credits_df[\"cast\"].astype(str).str.startswith(\"[\")][\"cast\"].unique())\n",
    "\n",
    "print(\"\\nCrew with suspicious values:\")\n",
    "print(credits_df[~credits_df[\"crew\"].astype(str).str.startswith(\"[\")][\"crew\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/1864447569.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"genres\"]= metadata_cleaned[\"genres\"].apply(lambda x: parse_list_column(x, \"name\"))\n"
     ]
    }
   ],
   "source": [
    "def parse_list_column(s, key=\"name\"):\n",
    "    try:\n",
    "        return [d[key] for d in ast.literal_eval(s)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "metadata_cleaned[\"genres\"]= metadata_cleaned[\"genres\"].apply(lambda x: parse_list_column(x, \"name\"))\n",
    "keywords_df[\"keywords\"]= keywords_df[\"keywords\"].apply(lambda x: parse_list_column(x, \"name\"))\n",
    "credits_df[\"cast\"]= credits_df[\"cast\"].apply(lambda x: parse_list_column(x, \"name\")[:5])  # top 5\n",
    "credits_df[\"crew\"]= credits_df[\"crew\"].apply(lambda x: parse_list_column(x, \"name\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         genres\n",
      "0   [Animation, Comedy, Family]\n",
      "1  [Adventure, Fantasy, Family]\n",
      "2             [Romance, Comedy]\n",
      "                                            keywords\n",
      "0  [jealousy, toy, boy, friendship, friends, riva...\n",
      "1  [board game, disappearance, based on children'...\n",
      "2  [fishing, best friend, duringcreditsstinger, o...\n",
      "                                                cast  \\\n",
      "0  [Tom Hanks, Tim Allen, Don Rickles, Jim Varney...   \n",
      "1  [Robin Williams, Jonathan Hyde, Kirsten Dunst,...   \n",
      "2  [Walter Matthau, Jack Lemmon, Ann-Margret, Sop...   \n",
      "\n",
      "                                                crew  \n",
      "0  [John Lasseter, Joss Whedon, Andrew Stanton, J...  \n",
      "1  [Larry J. Franco, Jonathan Hensleigh, James Ho...  \n",
      "2  [Howard Deutch, Mark Steven Johnson, Mark Stev...  \n"
     ]
    }
   ],
   "source": [
    "print(metadata_cleaned[[\"genres\"]].head(3))\n",
    "print(keywords_df[[\"keywords\"]].head(3))\n",
    "print(credits_df[[\"cast\", \"crew\"]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert data types (budget to int, release_date to datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "0    30000000\n",
      "1    65000000\n",
      "2           0\n",
      "3    16000000\n",
      "4           0\n",
      "5    60000000\n",
      "6    58000000\n",
      "7           0\n",
      "8    35000000\n",
      "9    58000000\n",
      "Name: budget, dtype: object\n",
      "object\n",
      "0    1995-10-30\n",
      "1    1995-12-15\n",
      "2    1995-12-22\n",
      "3    1995-12-22\n",
      "4    1995-02-10\n",
      "5    1995-12-15\n",
      "6    1995-12-15\n",
      "7    1995-12-22\n",
      "8    1995-12-22\n",
      "9    1995-11-16\n",
      "Name: release_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(metadata_cleaned[\"budget\"].dtypes)\n",
    "print(metadata_cleaned[\"budget\"].head(10))\n",
    "print(metadata_cleaned[\"release_date\"].dtypes)\n",
    "print(metadata_cleaned[\"release_date\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2956698593.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"budget\"]= safe_to_numeric(metadata_cleaned[\"budget\"],dtype=\"int64\")\n",
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2956698593.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"revenue\"] =safe_to_numeric(metadata_cleaned[\"revenue\"],dtype=\"int64\")\n",
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2956698593.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"popularity\"]= safe_to_numeric(metadata_cleaned[\"popularity\"],dtype=\"float32\")\n",
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2956698593.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"vote_average\"] = safe_to_numeric(metadata_cleaned[\"vote_average\"],dtype=\"float32\")\n",
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2956698593.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"vote_count\"] =safe_to_numeric(metadata_cleaned[\"vote_count\"],dtype=\"int32\")\n",
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/2956698593.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"runtime\"]= safe_to_numeric(metadata_cleaned[\"runtime\"],dtype=\"float32\")\n"
     ]
    }
   ],
   "source": [
    "def safe_to_numeric(series, dtype=\"float\"):\n",
    "    \"\"\"Convert to numeric safely, coerce errors, and fill NaN with 0.\"\"\"\n",
    "    return pd.to_numeric(series, errors=\"coerce\").fillna(0).astype(dtype)\n",
    "\n",
    "# Convert numeric columns\n",
    "metadata_cleaned[\"budget\"]= safe_to_numeric(metadata_cleaned[\"budget\"],dtype=\"int64\")\n",
    "metadata_cleaned[\"revenue\"] =safe_to_numeric(metadata_cleaned[\"revenue\"],dtype=\"int64\")\n",
    "metadata_cleaned[\"popularity\"]= safe_to_numeric(metadata_cleaned[\"popularity\"],dtype=\"float32\")\n",
    "metadata_cleaned[\"vote_average\"] = safe_to_numeric(metadata_cleaned[\"vote_average\"],dtype=\"float32\")\n",
    "metadata_cleaned[\"vote_count\"] =safe_to_numeric(metadata_cleaned[\"vote_count\"],dtype=\"int32\")\n",
    "metadata_cleaned[\"runtime\"]= safe_to_numeric(metadata_cleaned[\"runtime\"],dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    30000000\n",
      "1    65000000\n",
      "2           0\n",
      "3    16000000\n",
      "4           0\n",
      "5    60000000\n",
      "6    58000000\n",
      "7           0\n",
      "8    35000000\n",
      "9    58000000\n",
      "Name: budget, dtype: int64\n",
      "int64\n",
      "New NaN count: 0\n"
     ]
    }
   ],
   "source": [
    "print(metadata_cleaned[\"budget\"].head(10))\n",
    "print(metadata_cleaned[\"budget\"].dtype)\n",
    "print(\"New NaN count:\", metadata_cleaned[\"budget\"].isna().sum()) # No new NaNs introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "0   1995-10-30\n",
      "1   1995-12-15\n",
      "2   1995-12-22\n",
      "Name: release_date, dtype: datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/wkqtsjkx79xgl2mmhw0hd4000000gn/T/ipykernel_66166/306793262.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata_cleaned[\"release_date\"] = pd.to_datetime(metadata_cleaned[\"release_date\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "# Convert release_date to datetime\n",
    "metadata_cleaned[\"release_date\"] = pd.to_datetime(metadata_cleaned[\"release_date\"], errors=\"coerce\")\n",
    "print(metadata_cleaned[\"release_date\"].dtype)\n",
    "print(metadata_cleaned[\"release_date\"].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid release_date rows: 68 / 44481\n"
     ]
    }
   ],
   "source": [
    "# Count invalid dates (become NaT after coercion)\n",
    "invalid_dates = metadata_cleaned[\"release_date\"].isna().sum()\n",
    "total_rows = len(metadata_cleaned)\n",
    "\n",
    "print(f\"Invalid release_date rows: {invalid_dates} / {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing release_date\n",
    "metadata_cleaned = metadata_cleaned.dropna(subset=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Drop duplicates and invalid IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (metadata)\n",
      "Duplicates: 30\n",
      "Invalid IDs: 0\n",
      "Total rows: 44413\n",
      "\n",
      "After:\n",
      "Duplicates: 0\n",
      "Total rows: 44383\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: (metadata)\")\n",
    "print(\"Duplicates:\", metadata_cleaned[\"id\"].duplicated().sum())\n",
    "print(\"Invalid IDs:\", pd.to_numeric(metadata_cleaned[\"id\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Total rows:\", len(metadata_cleaned))\n",
    "\n",
    "# Clean\n",
    "metadata_cleaned[\"id\"] = pd.to_numeric(metadata_cleaned[\"id\"], errors=\"coerce\").astype(\"int64\")\n",
    "# metadata_cleaned = metadata_cleaned.dropna(subset=[\"id\"])\n",
    "metadata_cleaned = metadata_cleaned.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "print(\"\\nAfter:\")\n",
    "print(\"Duplicates:\", metadata_cleaned[\"id\"].duplicated().sum())\n",
    "# print(\"Invalid IDs:\", metadata_cleaned[\"id\"].isna().sum())\n",
    "print(\"Total rows:\", len(metadata_cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (credits):\n",
      "Duplicates: 44\n",
      "Invalid IDs: 0\n",
      "Total rows: 45476\n",
      "\n",
      "After (credits):\n",
      "Duplicates: 0\n",
      "Total rows: 45432\n"
     ]
    }
   ],
   "source": [
    "print(\"Before (credits):\")\n",
    "print(\"Duplicates:\", credits_df[\"id\"].duplicated().sum())\n",
    "print(\"Invalid IDs:\", pd.to_numeric(credits_df[\"id\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Total rows:\", len(credits_df))\n",
    "\n",
    "# Clean\n",
    "credits_df[\"id\"] = pd.to_numeric(credits_df[\"id\"], errors=\"coerce\").astype(\"int64\")\n",
    "# credits_df = credits_df.dropna(subset=[\"id\"])\n",
    "credits_df = credits_df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "print(\"\\nAfter (credits):\")\n",
    "print(\"Duplicates:\", credits_df[\"id\"].duplicated().sum())\n",
    "# print(\"Invalid IDs:\", credits_df[\"id\"].isna().sum())\n",
    "print(\"Total rows:\", len(credits_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (keywords):\n",
      "Duplicates: 987\n",
      "Invalid IDs: 0\n",
      "Total rows: 46419\n",
      "\n",
      "After (keywords):\n",
      "Duplicates: 0\n",
      "Total rows: 45432\n"
     ]
    }
   ],
   "source": [
    "print(\"Before (keywords):\")\n",
    "print(\"Duplicates:\", keywords_df[\"id\"].duplicated().sum())\n",
    "print(\"Invalid IDs:\", pd.to_numeric(keywords_df[\"id\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Total rows:\", len(keywords_df))\n",
    "\n",
    "# Clean\n",
    "keywords_df[\"id\"] = pd.to_numeric(keywords_df[\"id\"], errors=\"coerce\").astype(\"int64\")\n",
    "# keywords_df = keywords_df.dropna(subset=[\"id\"])\n",
    "keywords_df = keywords_df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "\n",
    "print(\"\\nAfter (keywords):\")\n",
    "print(\"Duplicates:\", keywords_df[\"id\"].duplicated().sum())\n",
    "# print(\"Invalid IDs:\", keywords_df[\"id\"].isna().sum())\n",
    "print(\"Total rows:\", len(keywords_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (ratings):\n",
      "Duplicates: 0\n",
      "Invalid movieIds: 0\n",
      "Total rows: 100004\n"
     ]
    }
   ],
   "source": [
    "print(\"Before (ratings):\")\n",
    "print(\"Duplicates:\", ratings_df.duplicated().sum())\n",
    "print(\"Invalid movieIds:\", pd.to_numeric(ratings_df[\"movieId\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Total rows:\", len(ratings_df))\n",
    "\n",
    "# Clean\n",
    "ratings_df[\"movieId\"] = pd.to_numeric(ratings_df[\"movieId\"], errors=\"coerce\").astype(\"int64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (links):\n",
      "Duplicates: 0\n",
      "Invalid movieId: 0\n",
      "Invalid imdbId: 0\n",
      "Invalid tmdbId: 0\n",
      "Total rows: 9112\n"
     ]
    }
   ],
   "source": [
    "print(\"Before (links):\")\n",
    "print(\"Duplicates:\", links_cleaned.duplicated().sum())\n",
    "print(\"Invalid movieId:\", pd.to_numeric(links_cleaned[\"movieId\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Invalid imdbId:\", pd.to_numeric(links_cleaned[\"imdbId\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Invalid tmdbId:\", pd.to_numeric(links_cleaned[\"tmdbId\"], errors=\"coerce\").isna().sum())\n",
    "print(\"Total rows:\", len(links_cleaned))\n",
    "\n",
    "# Clean\n",
    "links_cleaned[\"movieId\"] = pd.to_numeric(links_cleaned[\"movieId\"], errors=\"coerce\").astype(\"int64\")\n",
    "links_cleaned[\"imdbId\"] = pd.to_numeric(links_cleaned[\"imdbId\"], errors=\"coerce\").astype(\"int64\")\n",
    "links_cleaned[\"tmdbId\"] = pd.to_numeric(links_cleaned[\"tmdbId\"], errors=\"coerce\").astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_cleaned[\"tmdbId\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build a unified movies frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
      "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
      "       'popularity', 'poster_path', 'production_companies',\n",
      "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
      "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
      "       'vote_average', 'vote_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(metadata_cleaned.columns)\n",
    "movies_metadata= metadata_cleaned[[\"id\",\"title\",\"overview\",\"genres\",\"budget\",\"runtime\",\"release_date\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keywords'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(keywords_df.columns)\n",
    "credits_slim= credits_df[[\"id\", \"cast\", \"crew\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cast', 'crew', 'id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(credits_df.columns)\n",
    "keywords_slim= keywords_df[[\"id\", \"keywords\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genres</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>release_date</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>30000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>[Tom Hanks, Tim Allen, Don Rickles, Jim Varney...</td>\n",
       "      <td>[John Lasseter, Joss Whedon, Andrew Stanton, J...</td>\n",
       "      <td>[jealousy, toy, boy, friendship, friends, riva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "      <td>65000000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>[Robin Williams, Jonathan Hyde, Kirsten Dunst,...</td>\n",
       "      <td>[Larry J. Franco, Jonathan Hensleigh, James Ho...</td>\n",
       "      <td>[board game, disappearance, based on children'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>[Walter Matthau, Jack Lemmon, Ann-Margret, Sop...</td>\n",
       "      <td>[Howard Deutch, Mark Steven Johnson, Mark Stev...</td>\n",
       "      <td>[fishing, best friend, duringcreditsstinger, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>16000000</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>[Whitney Houston, Angela Bassett, Loretta Devi...</td>\n",
       "      <td>[Forest Whitaker, Ronald Bass, Ronald Bass, Ez...</td>\n",
       "      <td>[based on novel, interracial relationship, sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>[Steve Martin, Diane Keaton, Martin Short, Kim...</td>\n",
       "      <td>[Alan Silvestri, Elliot Davis, Nancy Meyers, N...</td>\n",
       "      <td>[baby, midlife crisis, confidence, aging, daug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title  \\\n",
       "0    862                    Toy Story   \n",
       "1   8844                      Jumanji   \n",
       "2  15602             Grumpier Old Men   \n",
       "3  31357            Waiting to Exhale   \n",
       "4  11862  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...   \n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "\n",
       "                         genres    budget  runtime release_date  \\\n",
       "0   [Animation, Comedy, Family]  30000000     81.0   1995-10-30   \n",
       "1  [Adventure, Fantasy, Family]  65000000    104.0   1995-12-15   \n",
       "2             [Romance, Comedy]         0    101.0   1995-12-22   \n",
       "3      [Comedy, Drama, Romance]  16000000    127.0   1995-12-22   \n",
       "4                      [Comedy]         0    106.0   1995-02-10   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [Tom Hanks, Tim Allen, Don Rickles, Jim Varney...   \n",
       "1  [Robin Williams, Jonathan Hyde, Kirsten Dunst,...   \n",
       "2  [Walter Matthau, Jack Lemmon, Ann-Margret, Sop...   \n",
       "3  [Whitney Houston, Angela Bassett, Loretta Devi...   \n",
       "4  [Steve Martin, Diane Keaton, Martin Short, Kim...   \n",
       "\n",
       "                                                crew  \\\n",
       "0  [John Lasseter, Joss Whedon, Andrew Stanton, J...   \n",
       "1  [Larry J. Franco, Jonathan Hensleigh, James Ho...   \n",
       "2  [Howard Deutch, Mark Steven Johnson, Mark Stev...   \n",
       "3  [Forest Whitaker, Ronald Bass, Ronald Bass, Ez...   \n",
       "4  [Alan Silvestri, Elliot Davis, Nancy Meyers, N...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [jealousy, toy, boy, friendship, friends, riva...  \n",
       "1  [board game, disappearance, based on children'...  \n",
       "2  [fishing, best friend, duringcreditsstinger, o...  \n",
       "3  [based on novel, interracial relationship, sin...  \n",
       "4  [baby, midlife crisis, confidence, aging, daug...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies= movies_metadata.merge(credits_slim, how=\"left\", on=\"id\").merge(keywords_slim, how=\"left\", on=\"id\")\n",
    "movies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in [\"genres\",\"cast\",\"crew\",\"keywords\"]:\n",
    "#     print(movies[column].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genres</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>release_date</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>30000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>[Tom Hanks, Tim Allen, Don Rickles, Jim Varney...</td>\n",
       "      <td>[John Lasseter, Joss Whedon, Andrew Stanton, J...</td>\n",
       "      <td>[jealousy, toy, boy, friendship, friends, riva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>[Adventure, Fantasy, Family]</td>\n",
       "      <td>65000000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>[Robin Williams, Jonathan Hyde, Kirsten Dunst,...</td>\n",
       "      <td>[Larry J. Franco, Jonathan Hensleigh, James Ho...</td>\n",
       "      <td>[board game, disappearance, based on children'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>[Romance, Comedy]</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>[Walter Matthau, Jack Lemmon, Ann-Margret, Sop...</td>\n",
       "      <td>[Howard Deutch, Mark Steven Johnson, Mark Stev...</td>\n",
       "      <td>[fishing, best friend, duringcreditsstinger, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             title                                           overview  \\\n",
       "0    862         Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
       "1   8844           Jumanji  When siblings Judy and Peter discover an encha...   \n",
       "2  15602  Grumpier Old Men  A family wedding reignites the ancient feud be...   \n",
       "\n",
       "                         genres    budget  runtime release_date  \\\n",
       "0   [Animation, Comedy, Family]  30000000     81.0   1995-10-30   \n",
       "1  [Adventure, Fantasy, Family]  65000000    104.0   1995-12-15   \n",
       "2             [Romance, Comedy]         0    101.0   1995-12-22   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [Tom Hanks, Tim Allen, Don Rickles, Jim Varney...   \n",
       "1  [Robin Williams, Jonathan Hyde, Kirsten Dunst,...   \n",
       "2  [Walter Matthau, Jack Lemmon, Ann-Margret, Sop...   \n",
       "\n",
       "                                                crew  \\\n",
       "0  [John Lasseter, Joss Whedon, Andrew Stanton, J...   \n",
       "1  [Larry J. Franco, Jonathan Hensleigh, James Ho...   \n",
       "2  [Howard Deutch, Mark Steven Johnson, Mark Stev...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [jealousy, toy, boy, friendship, friends, riva...  \n",
       "1  [board game, disappearance, based on children'...  \n",
       "2  [fishing, best friend, duringcreditsstinger, o...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for column in [\"genres\",\"cast\",\"crew\",\"keywords\"]:\n",
    "    movies[column]= movies[column].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release year as numeric\n",
    "movies[\"release_year\"] = movies[\"release_date\"].dt.year.fillna(0).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add flag for \"budget reported as zero\"\n",
    "# movies[\"budget_is_zero\"] = (movies[\"budget\"] == 0).astype(np.int8)\n",
    "\n",
    "# # Safe log transform: log1p(0) = 0, handles large budgets smoothly\n",
    "# movies[\"log_budget\"] = np.log1p(movies[\"budget\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text embeddings (Sentence-Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genres</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>release_date</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>keywords</th>\n",
       "      <th>release_year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>30000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>[Tom Hanks, Tim Allen, Don Rickles, Jim Varney...</td>\n",
       "      <td>[John Lasseter, Joss Whedon, Andrew Stanton, J...</td>\n",
       "      <td>[jealousy, toy, boy, friendship, friends, riva...</td>\n",
       "      <td>1995</td>\n",
       "      <td>Toy Story. Led by Woody, Andy's toys live happ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      title                                           overview  \\\n",
       "0  862  Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
       "\n",
       "                        genres    budget  runtime release_date  \\\n",
       "0  [Animation, Comedy, Family]  30000000     81.0   1995-10-30   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [Tom Hanks, Tim Allen, Don Rickles, Jim Varney...   \n",
       "\n",
       "                                                crew  \\\n",
       "0  [John Lasseter, Joss Whedon, Andrew Stanton, J...   \n",
       "\n",
       "                                            keywords  release_year  \\\n",
       "0  [jealousy, toy, boy, friendship, friends, riva...          1995   \n",
       "\n",
       "                                                text  \n",
       "0  Toy Story. Led by Woody, Andy's toys live happ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine title+overview=text\n",
    "def build_text(row):\n",
    "    title=(row.get(\"title\") or \"\").strip()\n",
    "    overview=(row.get(\"overview\") or \"\").strip()\n",
    "    return f\"{title}. {overview}\" if overview else title\n",
    "\n",
    "movies[\"text\"]= movies.apply(build_text, axis=1)\n",
    "movies.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8880c4198534bb8a716501ff73da878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding shape: (44383, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "text_model_name= \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "text_model = SentenceTransformer(text_model_name)\n",
    "\n",
    "# Batch-encode for speed\n",
    "texts= movies[\"text\"].tolist()\n",
    "text_embedding = text_model.encode(texts,batch_size=256,show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "print(\"Text embedding shape:\", text_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05570448  0.00022975  0.09581222 ...  0.03493924  0.04193446\n",
      "   0.06821849]\n",
      " [ 0.07180642  0.08725393 -0.02754332 ... -0.01197139 -0.06999753\n",
      "  -0.0225404 ]\n",
      " [-0.07744011  0.02054691 -0.00661253 ...  0.00445634 -0.03178686\n",
      "   0.01753086]\n",
      " ...\n",
      " [-0.02480008 -0.01578687 -0.01370617 ... -0.02934398 -0.00814912\n",
      "  -0.06585484]\n",
      " [-0.01156707  0.09120587 -0.06591202 ... -0.02868153 -0.00295109\n",
      "  -0.03940088]\n",
      " [-0.0468686   0.01742156 -0.09697933 ... -0.06267873 -0.01851477\n",
      "  -0.09517392]]\n"
     ]
    }
   ],
   "source": [
    "print(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Categorical vocabularies â†’ integer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(series_of_lists, top_k=None, add_unknown=True):\n",
    "    count = Counter()\n",
    "    for l in series_of_lists:\n",
    "        count.update([str(x).strip() for x in l if str(x).strip()])\n",
    "    # sort by freq desc\n",
    "    most = count.most_common(top_k) if top_k else count.most_common()\n",
    "    idx = 0\n",
    "    vocab = {}\n",
    "    if add_unknown:\n",
    "        vocab[\"<UNK>\"] = idx; \n",
    "        idx += 1\n",
    "    for token, j in most:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = idx; \n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "# Build vocabs\n",
    "genres_vocab= build_vocab(movies[\"genres\"],top_k=None)\n",
    "cast_vocab= build_vocab(movies[\"cast\"],top_k=5000)   # cap large lists\n",
    "crew_vocab= build_vocab(movies[\"crew\"],top_k=5000)\n",
    "keywords_vocab= build_vocab(movies[\"keywords\"], top_k=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(keywords_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres_ids</th>\n",
       "      <th>cast_ids</th>\n",
       "      <th>crew_ids</th>\n",
       "      <th>keywords_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>[14, 2, 11]</td>\n",
       "      <td>[58, 799, 2183, 1976, 927]</td>\n",
       "      <td>[56, 1421, 662, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[67, 1499, 337, 19, 93, 243, 6465, 8170, 1659]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>[9, 13, 11]</td>\n",
       "      <td>[49, 0, 453, 0, 1642]</td>\n",
       "      <td>[2611, 3729, 39, 4253, 0, 419, 0, 1880, 1172, ...</td>\n",
       "      <td>[3147, 357, 866, 2648, 3148, 1840]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>[4, 2]</td>\n",
       "      <td>[110, 111, 421, 258, 129]</td>\n",
       "      <td>[0, 2321, 2321, 4254]</td>\n",
       "      <td>[1187, 143, 16, 8171]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             title   genres_ids                    cast_ids  \\\n",
       "0    862         Toy Story  [14, 2, 11]  [58, 799, 2183, 1976, 927]   \n",
       "1   8844           Jumanji  [9, 13, 11]       [49, 0, 453, 0, 1642]   \n",
       "2  15602  Grumpier Old Men       [4, 2]   [110, 111, 421, 258, 129]   \n",
       "\n",
       "                                            crew_ids  \\\n",
       "0               [56, 1421, 662, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [2611, 3729, 39, 4253, 0, 419, 0, 1880, 1172, ...   \n",
       "2                              [0, 2321, 2321, 4254]   \n",
       "\n",
       "                                     keywords_ids  \n",
       "0  [67, 1499, 337, 19, 93, 243, 6465, 8170, 1659]  \n",
       "1              [3147, 357, 866, 2648, 3148, 1840]  \n",
       "2                           [1187, 143, 16, 8171]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_list(l, vocab):\n",
    "    return [vocab.get(str(x).strip(), vocab.get(\"<UNK>\", 0)) for x in l if str(x).strip()]\n",
    "\n",
    "movies[\"genres_ids\"]= movies[\"genres\"].apply(lambda x: map_list(x, genres_vocab))\n",
    "movies[\"cast_ids\"]= movies[\"cast\"].apply(lambda x: map_list(x, cast_vocab)[:10])     # cap per-movie length\n",
    "movies[\"crew_ids\"]= movies[\"crew\"].apply(lambda x: map_list(x, crew_vocab)[:10])\n",
    "movies[\"keywords_ids\"] = movies[\"keywords\"].apply(lambda x: map_list(x, keywords_vocab)[:20])\n",
    "\n",
    "# Quick peek\n",
    "movies[[\"id\",\"title\",\"genres_ids\",\"cast_ids\",\"crew_ids\",\"keywords_ids\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Numeric features normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features shape: (44383, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_columns = [\"budget\", \"runtime\", \"release_year\"]\n",
    "scaler= StandardScaler()\n",
    "num_features = scaler.fit_transform(movies[num_columns].values.astype(\"float32\"))\n",
    "print(\"Numeric features shape:\",num_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4574289 , -0.37442946,  0.13250543],\n",
       "       [ 3.4438074 ,  0.23886582,  0.13250543],\n",
       "       [-0.24518108,  0.15887079,  0.13250543],\n",
       "       ...,\n",
       "       [-0.24518108, -0.13444434,  0.4640645 ],\n",
       "       [-0.24518108, -0.21443938, -3.1001956 ],\n",
       "       [-0.24518108, -0.5344195 ,  1.0442929 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Package features + save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ['movie_embeddings_finetuned.npz', 'content_feature_artifacts.pkl', 'movie_faiss_index_flat.index', 'movie_embeddings.npz', 'categorical_id_lists.parquet', 'content_features_npz.npz']\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "\n",
    "# setup\n",
    "features_dir = \"./features_artifacts\"\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "tmdb_ids = movies[\"id\"].values.astype(\"int64\")\n",
    "\n",
    "# Save numerical/text features\n",
    "np.savez_compressed(\n",
    "    os.path.join(features_dir, \"content_features_npz.npz\"),\n",
    "    tmdb_id = tmdb_ids,\n",
    "    text_emb = text_embedding.astype(\"float32\"),\n",
    "    num_feats = num_features.astype(\"float32\"),\n",
    ")\n",
    "\n",
    "# save auxiliary artifacts\n",
    "artifacts = {\n",
    "    \"genres_vocab\": genres_vocab,\n",
    "    \"cast_vocab\": cast_vocab,\n",
    "    \"crew_vocab\": crew_vocab,\n",
    "    \"keywords_vocab\": keywords_vocab,\n",
    "    \"scaler_num\": scaler,\n",
    "    \"text_model\": text_model_name,\n",
    "    \"num_cols\": num_columns,\n",
    "}\n",
    "with open(os.path.join(features_dir, \"content_feature_artifacts.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "\n",
    "# save categorical ID lists\n",
    "id_lists = movies[[\"id\",\"genres_ids\",\"cast_ids\",\"crew_ids\",\"keywords_ids\"]].copy()\n",
    "id_lists.to_parquet(os.path.join(features_dir, \"categorical_id_lists.parquet\"), index=False)\n",
    "\n",
    "print(\"Saved:\", os.listdir(features_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Model\n",
    "### 3.0 Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the features\n",
    "npz = np.load(os.path.join(features_dir,\"content_features_npz.npz\"))\n",
    "tmdb_ids= npz[\"tmdb_id\"]          # [N]\n",
    "text_emb= npz[\"text_emb\"]         # [N, 384] for MiniLM\n",
    "num_feats= npz[\"num_feats\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load categorical IDs, vocabularies, scalers, metadata\n",
    "id_lists= pd.read_parquet(os.path.join(features_dir,\"categorical_id_lists.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44383, 384) (44383, 3) (44383, 5)\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(features_dir, \"content_feature_artifacts.pkl\"), \"rb\") as f:\n",
    "    art= pickle.load(f)\n",
    "\n",
    "genres_vocab= art[\"genres_vocab\"]\n",
    "cast_vocab= art[\"cast_vocab\"]\n",
    "crew_vocab= art[\"crew_vocab\"]\n",
    "keywords_vocab= art[\"keywords_vocab\"]\n",
    "\n",
    "# recover vocab sizes\n",
    "V_G= max(genres_vocab.values())+1\n",
    "V_C= max(cast_vocab.values())+1\n",
    "V_R= max(crew_vocab.values())+1\n",
    "V_K= max(keywords_vocab.values())+1\n",
    "\n",
    "print(text_emb.shape, num_feats.shape, id_lists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset: anchors + positives + negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build inverted indices for e.g. movies and genres\n",
    "def build_token_index(series_of_lists):\n",
    "    token_to_idx= defaultdict(set)\n",
    "    for idx, lst in enumerate(series_of_lists):\n",
    "        for token in lst:\n",
    "            token_to_idx[int(token)].add(idx)\n",
    "    return token_to_idx\n",
    "\n",
    "# for e.g. inv_genres = {1: {0,2}, 2: {0}, 3: {1}, 4: {1}, 5: {2}}\n",
    "# Meaning: Genre 1 appears in movies 0 and 2.\n",
    "# The argument series_of_lists is aligned with the row index of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_ids= id_lists[\"genres_ids\"].tolist()\n",
    "cast_ids= id_lists[\"cast_ids\"].tolist()\n",
    "crew_ids= id_lists[\"crew_ids\"].tolist()\n",
    "keywords_ids= id_lists[\"keywords_ids\"].to_list()\n",
    "\n",
    "inv_genres= build_token_index(genre_ids)\n",
    "# inv_cast= build_token_index(cast_ids)\n",
    "# inv_crew= build_token_index(crew_ids)\n",
    "inv_keywords= build_token_index(keywords_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_from_inv(idx):\n",
    "    # make a union of same-genre or same-keyword neighbors (exclude/drop self)\n",
    "    candidates = set()\n",
    "    for g in genre_ids[idx]:\n",
    "        candidates |= inv_genres.get(g, set())\n",
    "    for k in keywords_ids[idx]:\n",
    "        candidates |= inv_keywords.get(k, set())\n",
    "    candidates.discard(idx)  # drop self\n",
    "    return list(candidates)\n",
    "\n",
    "# Example if movie 0 is â€œRush Hourâ€ (Action, Comedy), positives will be any other Action or Comedy movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviePairs(Dataset):\n",
    "    def __init__(self, text_emb, num_feats, genre_ids, cast_ids, crew_ids, keywords_ids):\n",
    "        # Initializes the dataset with text embeddings, numeric features, and categorical ID lists.\n",
    "        # Also sets up references to genre, cast, crew, and keyword ID lists, and stores the total number of movies.\n",
    "        self.num_feats = num_feats\n",
    "        self.text_emb = text_emb\n",
    "        self.g = genre_ids\n",
    "        self.c = cast_ids\n",
    "        self.r = crew_ids\n",
    "        self.k = keywords_ids\n",
    "        self.N = len(text_emb)\n",
    "        self.all_idx = list(range(self.N))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of movies (dataset size).\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # Given an index i (anchor movie), selects:\n",
    "        # - a positive sample: another movie sharing at least one genre or keyword (if available), otherwise random\n",
    "        # - a negative sample: a random movie different from anchor and positive\n",
    "        # Returns a dictionary with features for anchor, positive, and negative movies.\n",
    "        anchor = i\n",
    "        pos_candidates = candidates_from_inv(anchor)\n",
    "        if pos_candidates:\n",
    "            positive = random.choice(pos_candidates)\n",
    "        else:\n",
    "            positive = random.randrange(self.N)\n",
    "\n",
    "        negative = random.randrange(self.N)\n",
    "        while negative == anchor or negative == positive:\n",
    "            negative = random.randrange(self.N)\n",
    "    \n",
    "        return {\n",
    "            \"anchor_text\": self.text_emb[anchor],\n",
    "            \"anchor_num\": self.num_feats[anchor],\n",
    "            \"anchor_g\": list(self.g[anchor]),\n",
    "            \"anchor_c\": list(self.c[anchor]),\n",
    "            \"anchor_r\": list(self.r[anchor]),\n",
    "            \"anchor_k\": list(self.k[anchor]),\n",
    "            \"positive_text\": self.text_emb[positive],\n",
    "            \"positive_num\": self.num_feats[positive],\n",
    "            \"positive_g\": list(self.g[positive]),\n",
    "            \"positive_c\": list(self.c[positive]),\n",
    "            \"positive_r\": list(self.r[positive]),\n",
    "            \"positive_k\": list(self.k[positive]),\n",
    "            \"negative_text\": self.text_emb[negative],\n",
    "            \"negative_num\": self.num_feats[negative],\n",
    "            \"negative_g\": list(self.g[negative]),\n",
    "            \"negative_c\": list(self.c[negative]),\n",
    "            \"negative_r\": list(self.r[negative]),\n",
    "            \"negative_k\": list(self.k[negative]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['anchor_text', 'anchor_num', 'anchor_g', 'anchor_c', 'anchor_r', 'anchor_k', 'positive_text', 'positive_num', 'positive_g', 'positive_c', 'positive_r', 'positive_k', 'negative_text', 'negative_num', 'negative_g', 'negative_c', 'negative_r', 'negative_k'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate(batch):\n",
    "    # Convert to tensors; lists stay as Python lists (pad inside the model)\n",
    "    to_tensor= lambda x: torch.tensor(np.stack(x), dtype=torch.float32)\n",
    "    pack= {}\n",
    "    for prefix in [\"anchor\", \"positive\", \"negative\"]:\n",
    "        pack[f\"{prefix}_text\"]= to_tensor([item[f\"{prefix}_text\"] for item in batch])\n",
    "        pack[f\"{prefix}_num\"]= to_tensor([item[f\"{prefix}_num\"] for item in batch])\n",
    "        for feat in [\"g\",\"c\",\"r\",\"k\"]:\n",
    "            pack[f\"{prefix}_{feat}\"]= [item[f\"{prefix}_{feat}\"] for item in batch]\n",
    "    return pack\n",
    "\n",
    "# DataLoader with batching (256) and shuffling (of each epoch)- creates a dataset of triplets (anchor, positive, negative)\n",
    "dataset= MoviePairs(text_emb, num_features, genre_ids, cast_ids, crew_ids, keywords_ids)\n",
    "dataloader= DataLoader(dataset, batch_size=256, shuffle=True, num_workers=0, collate_fn=collate)   \n",
    "next(iter(dataloader)).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model: embeddings + mean pooling â†’ 128-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagEmbed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, lists_of_ids):\n",
    "        B= len(lists_of_ids)\n",
    "        maxlen= max((len(l) for l in lists_of_ids), default=1)\n",
    "        device= self.embedding.weight.device\n",
    "        input_tensor= torch.zeros((B, maxlen), dtype=torch.long, device=device)\n",
    "        lens=[]\n",
    "        for i, l in enumerate(lists_of_ids):\n",
    "            lens.append(len(l))\n",
    "            if len(l) > 0:\n",
    "                input_tensor[i, :len(l)] = torch.tensor(l, dtype=torch.long, device=device)\n",
    "        embeds= self.embedding(input_tensor)  # [B, maxlen, D]\n",
    "        mask= (input_tensor != 0).float().unsqueeze(-1)  # [B, maxlen, 1]\n",
    "        pooled= (embeds * mask).sum(dim=1) / (mask.sum(dim=1).clamp_min(1.0))  # mean pool\n",
    "        return pooled  # [B, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentTower(nn.Module):\n",
    "    def __init__(self, dims, vocab_sizes):\n",
    "        super().__init__()\n",
    "        self.genres= BagEmbed(vocab_sizes[\"genres\"], dims[\"cat\"])\n",
    "        self.cast= BagEmbed(vocab_sizes[\"cast\"], dims[\"cat\"])\n",
    "        self.crew= BagEmbed(vocab_sizes[\"crew\"], dims[\"cat\"])\n",
    "        self.keywords= BagEmbed(vocab_sizes[\"keywords\"], dims[\"cat\"])\n",
    "\n",
    "        d_in= dims[\"text\"] + dims[\"num\"] + 4 * dims[\"cat\"]\n",
    "        self.mlp= nn.Sequential(\n",
    "            nn.Linear(d_in, 256), nn.ReLU(),\n",
    "            nn.Linear(256, dims[\"out\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, text_emb, num_feat, g_ids, c_ids, r_ids, k_ids):\n",
    "        g_emb= self.genres(g_ids)\n",
    "        c_emb= self.cast(c_ids)\n",
    "        r_emb= self.crew(r_ids)\n",
    "        k_emb= self.keywords(k_ids)\n",
    "        x= torch.cat([text_emb, num_feat, g_emb, c_emb, r_emb, k_emb], dim=1)\n",
    "        z= F.normalize(self.mlp(x), p=2, dim=1)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims= {\"text\": text_emb.shape[1], \"num\": num_features.shape[1], \"cat\": 64, \"out\": 128}\n",
    "vocab_sizes= {\"genres\": V_G, \"cast\": V_C, \"crew\": V_R, \"keywords\": V_K}\n",
    "model= ContentTower(dims, vocab_sizes).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loss: choose one (cosine or triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Triplet Margin Loss\n",
    "# loss_fn= nn.TripletMarginLoss(margin=0.2, p=2)\n",
    "\n",
    "# Option B: Custom Cosine Pair Loss (cosine similarity loss)\n",
    "def cosine_pair_loss(a, p, n, margin=0.2):\n",
    "    # cos sim in [-1, 1]\n",
    "    cos_ap= (a*p).sum(dim=1)  # [B]\n",
    "    cos_an= (a*n).sum(dim=1)\n",
    "    # hinge: (margin- ap+ an)\n",
    "    loss= torch.clamp(margin - cos_ap + cos_an, min=0.0).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 50 Avg Loss: 0.1357\n",
      "Epoch 1 Step 100 Avg Loss: 0.0591\n",
      "Epoch 1 Step 150 Avg Loss: 0.0368\n",
      "Epoch 2 Step 50 Avg Loss: 0.1069\n",
      "Epoch 2 Step 100 Avg Loss: 0.0537\n",
      "Epoch 2 Step 150 Avg Loss: 0.0360\n",
      "Epoch 3 Step 50 Avg Loss: 0.1021\n",
      "Epoch 3 Step 100 Avg Loss: 0.0533\n",
      "Epoch 3 Step 150 Avg Loss: 0.0350\n",
      "Model saved to ./checkpoints/content_tower.pt\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS=3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss= 0.0\n",
    "    for step, batch in enumerate(dataloader, 1):\n",
    "        a= model(\n",
    "            batch[\"anchor_text\"].to(device),\n",
    "            batch[\"anchor_num\"].to(device),\n",
    "            batch[\"anchor_g\"],\n",
    "            batch[\"anchor_c\"],\n",
    "            batch[\"anchor_r\"],\n",
    "            batch[\"anchor_k\"],\n",
    "        )\n",
    "        p= model(\n",
    "            batch[\"positive_text\"].to(device),\n",
    "            batch[\"positive_num\"].to(device),\n",
    "            batch[\"positive_g\"],\n",
    "            batch[\"positive_c\"],\n",
    "            batch[\"positive_r\"],\n",
    "            batch[\"positive_k\"],\n",
    "        )\n",
    "        n= model(\n",
    "            batch[\"negative_text\"].to(device),\n",
    "            batch[\"negative_num\"].to(device),\n",
    "            batch[\"negative_g\"],\n",
    "            batch[\"negative_c\"],\n",
    "            batch[\"negative_r\"],\n",
    "            batch[\"negative_k\"],\n",
    "        )\n",
    "        loss= cosine_pair_loss(a, p, n, margin=0.2) # or triplet(a,p,n)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            avg_loss= total_loss / step\n",
    "            print(f\"Epoch {epoch+1} Step {step} Avg Loss: {avg_loss:.4f}\")\n",
    "            total_loss= 0.0\n",
    "# End of epoch\n",
    "\n",
    "# Save the trained model weights\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"./checkpoints/content_tower.pt\")\n",
    "print(\"Model saved to ./checkpoints/content_tower.pt\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Export the movie embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie embeddings shape: (44383, 128)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "B= 2048  # batch size for inference\n",
    "all_embeddings= []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(tmdb_ids), B):\n",
    "        Z = model(\n",
    "            torch.tensor(text_emb[i:i+B], dtype=torch.float32, device=device),\n",
    "            torch.tensor(num_features[i:i+B], dtype=torch.float32, device=device),\n",
    "            genre_ids[i:i+B],\n",
    "            cast_ids[i:i+B],\n",
    "            crew_ids[i:i+B],\n",
    "            keywords_ids[i:i+B]\n",
    "        )\n",
    "        all_embeddings.append(Z.cpu().numpy())\n",
    "\n",
    "movie_embeddings= np.vstack(all_embeddings).astype(\"float32\")  # [N, 128]\n",
    "print(\"Movie embeddings shape:\", movie_embeddings.shape)\n",
    "\n",
    "np.savez_compressed(os.path.join(features_dir, \"movie_embeddings.npz\"), tmdb_id=tmdb_ids, emb=movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Quick retrieval test (no FAISS yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of TMDB ID 862 (Toy Story): [   863    771 196633   9969  37556  44157  31309  16508  36251  73541] Scores: [0.9179871  0.8985822  0.89654386 0.8953173  0.89306736 0.88762194\n",
      " 0.8855413  0.88159513 0.8811209  0.8805306 ]\n"
     ]
    }
   ],
   "source": [
    "def topk_neighbors(seed_idx, k=10):\n",
    "    sims= movie_embeddings @ movie_embeddings[seed_idx] # since both [D], [N, D] are L2-normalized, cosine=dot product (=[N])\n",
    "    sims[seed_idx] = -1.0                   # exclude self\n",
    "    nbrs= np.argpartition(-sims, k)[:k]\n",
    "    nbrs= nbrs[np.argsort(-sims[nbrs])]\n",
    "    return nbrs, sims[nbrs]\n",
    "\n",
    "seed_idx= int(np.where(tmdb_ids==862)[0][0]) # find index of TMDB ID 862 (Toy Story)\n",
    "nbrs, scores= topk_neighbors(seed_idx, k=10)\n",
    "print(\"Neighbors of TMDB ID 862 (Toy Story):\", tmdb_ids[nbrs], \"Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    863  Toy Story 2  cos=0.918\n",
      "    771  Home Alone  cos=0.899\n",
      " 196633  Botsman i Popugay  cos=0.897\n",
      "   9969  Deck the Halls  cos=0.895\n",
      "  37556  Charlie Brown's Christmas Tales  cos=0.893\n",
      "  44157  How To Frame A Figg  cos=0.888\n",
      "  31309  Mr. Bug Goes to Town  cos=0.886\n",
      "  16508  Doug's 1st Movie  cos=0.882\n",
      "  36251  Switching Goals  cos=0.881\n",
      "  73541  You Should Meet My Son  cos=0.881\n"
     ]
    }
   ],
   "source": [
    "# Map to titles for a quick eyeball check\n",
    "# Build once:\n",
    "id_to_title = dict(zip(movies[\"id\"].astype(int).values, movies[\"title\"].astype(str).values))\n",
    "\n",
    "for i, s in zip(nbrs, scores):\n",
    "    mid = int(tmdb_ids[i])\n",
    "    print(f\"{mid:>7}  {id_to_title.get(mid, '?')}  cos={float(s):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment & slicing gotchas (very important)\n",
    "# Make sure the arrays line up 1:1\n",
    "\n",
    "assert movie_embeddings.shape[0] == len(tmdb_ids)\n",
    "genres_ids= id_lists[\"genres_ids\"].tolist()\n",
    "cast_ids= id_lists[\"cast_ids\"].tolist()\n",
    "crew_ids= id_lists[\"crew_ids\"].tolist()\n",
    "keywords_ids= id_lists[\"keywords_ids\"].tolist()\n",
    "\n",
    "# Sanity: the seed truly corresponds to TMDB 862\n",
    "assert tmdb_ids[seed_idx] == 862\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brave sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data= np.load(\"./features_artifacts/movie_embeddings.npz\")\n",
    "# emb= data[\"emb\"]\n",
    "# tmdb= data[\"tmdb_id\"]  \n",
    "\n",
    "# # Load ID -> title map\n",
    "# with open(\"./features_artifacts/content_feature_artifacts.pkl\", \"rb\") as f:\n",
    "#     art= pickle.load(f)\n",
    "\n",
    "# print(type(art))\n",
    "# if isinstance(art, dict):\n",
    "#     print(\"Dict keys:\", list(art.keys())[:10])\n",
    "# elif hasattr(art, \"head\"):  # probably a pandas DataFrame\n",
    "#     print(art.head())\n",
    "# else:\n",
    "#     print(\"First element:\", str(art)[:500])\n",
    "\n",
    "\n",
    "# def show_neighbors(seed_id, topk=10):\n",
    "#     idx = np.where(tmdb == seed_id)[0][0]\n",
    "#     sims = emb @ emb[idx]  # cosine similarity since L2-normalized\n",
    "#     nn_idx = sims.argsort()[::-1][1:topk+1]  # exclude self\n",
    "#     print(\"Seed:\", id_to_title.get(int(seed_id), str(seed_id)))\n",
    "#     for j in nn_idx:\n",
    "#         mid= int(tmdb_ids[j])\n",
    "#         print(f\" {mid:>7} {id_to_title.get(mid, '?'):40s} cos={sims[j]:.3f}\")\n",
    "\n",
    "\n",
    "# # SPot check across genres\n",
    "# show_neighbors(862) # Toy Story\n",
    "# show_neighbors(550) # Fight Club â†’ should find similar dark dramas\n",
    "# show_neighbors(603) # The Matrix â†’ should find sci-fi / action\n",
    "# show_neighbors(13) # Forrest Gump â†’ should find dramas/romance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neighbor_overlap(idx, field, topk=10):\n",
    "#     \"\"\"Compute how often the neighbors share at least one genre/keyword with the seed.\"\"\"\n",
    "#     sims= emb @ emb[idx]\n",
    "#     nn_idx= sims.argsort()[::-1][1:topk+1]  # exclude self\n",
    "#     target_set= set(id_lists.iloc[idx][field])\n",
    "#     overlaps=[]\n",
    "#     for j in nn_idx:\n",
    "#         nbr_set= set(id_lists.iloc[j][field])\n",
    "#         overlaps.append(1 if target_set & nbr_set else 0)\n",
    "\n",
    "#     return np.mean(overlaps)\n",
    "    \n",
    "# # Sample 100 seeds at random\n",
    "# random.seed(42)\n",
    "# sample_idx= random.choices(range(len(tmdb)), k=100)\n",
    "# mean_genre_overlap= np.mean([neighbor_overlap(i, \"genres_ids\", topk=10) for i in sample_idx])\n",
    "# print(f\"Mean genre overlap in top 10 neighbors: {mean_genre_overlap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean genre overlap in top 10 neighbors: 0.920\n",
    "# This is very high (92%) â†’ almost all top-10 neighbors share at least one genre.\n",
    "\n",
    "# Confirms the embeddings are clustering movies correctly in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_title = dict(zip(movies[\"id\"], movies[\"title\"]))\n",
    "# with open(\"./features_artifacts/id_to_title.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(id_to_title, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Map IDs (stable indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings after cleaning: (99796, 6)\n",
      "Unique users: 671\n",
      "Unique movies: 9013\n"
     ]
    }
   ],
   "source": [
    "# ratings_df already loaded/cleaned in Step 1\n",
    "u2i = {u: i for i, u in enumerate(sorted(ratings_df[\"userId\"].unique()))}\n",
    "# Map MovieLens movieID -> TMDB index from Step 3\n",
    "links_map= links_cleaned.set_index(\"movieId\")[\"tmdbId\"].to_dict()\n",
    "tmdb_to_row= {int(t): i for i, t in enumerate(tmdb_ids)} # row index in movie_embeddings\n",
    "\n",
    "def mlid_to_row(movieId):\n",
    "    \"\"\"Map MovieLens movieId to row index in movie_embeddings (or None if not found).\"\"\"\n",
    "    tmdb_id = links_map.get(int(movieId))\n",
    "    if tmdb_id is None:\n",
    "        return None\n",
    "    return tmdb_to_row.get(int(tmdb_id))\n",
    "\n",
    "ratings_df[\"user_idx\"] = ratings_df[\"userId\"].map(u2i)\n",
    "ratings_df[\"movie_idx\"] = ratings_df[\"movieId\"].map(mlid_to_row)\n",
    "ratings_cleaned = ratings_df.dropna(subset=[\"movie_idx\"]).assign(movie_idx=lambda x: x[\"movie_idx\"].astype(\"int32\"))\n",
    "print(\"Ratings after cleaning:\", ratings_cleaned.shape)\n",
    "print(\"Unique users:\", ratings_cleaned[\"userId\"].nunique())\n",
    "print(\"Unique movies:\", ratings_cleaned[\"movieId\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Leave-last-1 per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratings: (99125, 6)\n",
      "Test ratings: (671, 6)\n"
     ]
    }
   ],
   "source": [
    "if 'timestamp' in ratings_cleaned.columns:\n",
    "    ratings_cleaned = ratings_cleaned.sort_values(['userId', 'timestamp'])\n",
    "else:\n",
    "    ratings_cleaned = ratings_cleaned.sort_values(['userId']) # sort by userId only if timestamp is missing, fallback\n",
    "\n",
    "test_idx= ratings_cleaned.groupby(\"userId\").tail(1).index\n",
    "train_df= ratings_cleaned.drop(index=test_idx)\n",
    "test_df= ratings_cleaned.loc[test_idx]\n",
    "print(\"Train ratings:\", train_df.shape)\n",
    "print(\"Test ratings:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positive ratio: 0.51513743\n",
      "Test positive ratio: 0.55588675\n"
     ]
    }
   ],
   "source": [
    "train_df['y'] = (train_df['rating'] >= 4.0).astype('float32')  # threshold at 4.0\n",
    "test_df['y'] = (test_df['rating'] >= 4.0).astype('float32')  # threshold at 4.0\n",
    "print(\"Train positive ratio:\", train_df['y'].mean())\n",
    "print(\"Test positive ratio:\", test_df['y'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Imports & data shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 671\n",
      "Number of movies: 44383\n",
      "Number of items: 44383\n",
      "Embedding dimension: 128\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_users= len(u2i)\n",
    "num_movies= len(tmdb_ids)\n",
    "print(\"Number of users:\", num_users)\n",
    "print(\"Number of movies:\", num_movies) \n",
    "num_items= movie_embeddings.shape[0] #rows correspond 1:1 to tmdb_ids\n",
    "emb_dim= movie_embeddings.shape[1] # 128 from content tower\n",
    "print(\"Number of items:\", num_items)\n",
    "print(\"Embedding dimension:\", emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_idx= df[\"user_idx\"].values.astype(\"int32\")\n",
    "        self.movie_idx= df[\"movie_idx\"].values.astype(\"int32\")\n",
    "        self.y= df[\"y\"].values.astype(\"float32\")\n",
    "        # self.N= len(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return self.N\n",
    "        return len(self.user_idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.user_idx[i], self.movie_idx[i], self.y[i]\n",
    "    \n",
    "\n",
    "train_ds= RatingsDataset(train_df)\n",
    "test_ds= RatingsDataset(test_df)\n",
    "\n",
    "trainloader= DataLoader(train_ds, batch_size=1024, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 NCF Model (movie emedding initialized from Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ncf_model import NCF\n",
    "    \n",
    "# Stage-1: freeze item embeddings\n",
    "model= NCF(num_users, num_items, emb_dim, movie_embeddings, freeze_items=True, user_dim=emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Train (Stage-1 , BCEWithLogitsLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, loss_fn):\n",
    "    model.train()\n",
    "    total, step_loss= 0.0, 0\n",
    "    for user_idx, movie_idx, y in loader:\n",
    "        user_idx= user_idx.long().to(device)\n",
    "        movie_idx= movie_idx.long().to(device)\n",
    "        y= y.to(device)\n",
    "\n",
    "        logits= model(user_idx, movie_idx)\n",
    "        loss= loss_fn(logits, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        step_loss += 1\n",
    "    return total / max(1, step_loss)\n",
    "\n",
    "loss_fn= nn.BCEWithLogitsLoss()\n",
    "opt= torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = trainloader.dataset[0]\n",
    "# print(type(sample), sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-1] Epoch 1 Train Loss BCE: 0.6215\n",
      "[Stage-1] Epoch 2 Train Loss BCE: 0.6006\n",
      "[Stage-1] Epoch 3 Train Loss BCE: 0.5957\n"
     ]
    }
   ],
   "source": [
    "# lower the LR â€” from 1e-3 â†’ 5e-4 or 1e-4 if there is instability\n",
    "for epoch in range(3):\n",
    "    avg_loss= train_one_epoch(model, trainloader, opt, loss_fn)\n",
    "    print(f\"[Stage-1] Epoch {epoch+1} Train Loss BCE: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Evaluation: Hit@K and NDCG@K (leave-last-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId         int64\n",
       "movieId        int64\n",
       "rating       float64\n",
       "timestamp      int64\n",
       "user_idx       int64\n",
       "movie_idx      int32\n",
       "y            float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test users: 671\n",
      "Example test user->item: [(0, 1116), (1, 398), (2, 711), (3, 2320), (4, 3877)]\n"
     ]
    }
   ],
   "source": [
    "# Build \"seen items\" per user from training set (to filter out during ranking)\n",
    "user_seen_items= defaultdict(set)\n",
    "for u, m in zip(train_df[\"user_idx\"].values, train_df[\"movie_idx\"].values):\n",
    "    user_seen_items[int(u)].add(int(m))\n",
    "\n",
    "# Map test users -> (single) test item (leave-last-1)\n",
    "test_user_to_item= dict(zip(test_df[\"user_idx\"].values, test_df[\"movie_idx\"].values))\n",
    "\n",
    "print(\"Test users:\", len(test_user_to_item))\n",
    "print(\"Example test user->item:\", list(test_user_to_item.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_for_user(model, user_idx, true_m_idx, K=10, batch_items=4096):\n",
    "    \"\"\"Return rank (1-based) of true item among all unseen items. If filtered, returns None.\"\"\"\n",
    "    model.eval()\n",
    "    u= torch.tensor([user_idx], dtype=torch.long, device=device)\n",
    "    seen_items= user_seen_items.get(user_idx, set())\n",
    "\n",
    "    # cabdidates = all items excluding seen items\n",
    "    candidates= np.array([i for i in range(num_items) if i not in seen_items], dtype=np.int32)\n",
    "\n",
    "    if len(candidates) == 0 or true_m_idx not in candidates:\n",
    "        return None  # no candidates to rank or true item already seen (edge case)\n",
    "\n",
    "    scores= []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(candidates), batch_items):\n",
    "            batch = candidates[i:i + batch_items]\n",
    "            batch_u = u.repeat(len(batch))  # [B]\n",
    "            batch_m = torch.tensor(batch, dtype=torch.long, device=device)\n",
    "            logits = model(batch_u, batch_m)\n",
    "            scores.append(logits.cpu().numpy())\n",
    "    scores= np.concatenate(scores, axis=0)  # [num_candidates]\n",
    "\n",
    "    # Rank candidates by score (higher is better)\n",
    "    ranked_indices= np.argsort(-scores)     # indices of candidates sorted by score desc\n",
    "    ranked_items= candidates[ranked_indices]  # Numpy array of ranked item indices\n",
    "\n",
    "    # 1-based rank of true item\n",
    "    rank= np.where(ranked_items == true_m_idx)[0]\n",
    "    return int(rank[0]) + 1 if len(rank) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage1] Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_hit_ndcg(model, K=10):\n",
    "    hits, ndcgs, total= 0, 0.0, 0\n",
    "    for u, m in test_user_to_item.items():\n",
    "        rank= rank_for_user(model, u, m, K=K)\n",
    "        if rank is None:\n",
    "            continue\n",
    "        total += 1\n",
    "        if rank <= K:\n",
    "            hits += 1\n",
    "            ndcgs += 1.0 / np.log2(rank + 1)\n",
    "    hitk= hits / max(1, total)\n",
    "    ndcgk= ndcgs / max(1, total)\n",
    "    return hitk, ndcgk, total\n",
    "\n",
    "hit10, ndcg10, n_eval= evaluate_hit_ndcg(model, K=10)\n",
    "print(f\"[Stage1] Users evaluated: {n_eval}, Hit@10: {hit10:.4f}, NDCG@10: {ndcg10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Stage-2: unfreeze items & fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] Epoch 1 Train Loss BCE: 0.5671 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "[Stage-2] Epoch 2 Train Loss BCE: 0.5162 Users evaluated: 671, Hit@10: 0.0015, NDCG@10: 0.0005\n",
      "[Stage-2] Epoch 3 Train Loss BCE: 0.4756 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze item embeddings for fine-tuning\n",
    "model.item_emb.weight.requires_grad = True\n",
    "opt= torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)  # lower LR for fine-tuning\n",
    "\n",
    "for epoch in range(3):\n",
    "    tr_loss= train_one_epoch(model, trainloader, opt, loss_fn)\n",
    "    hit10, ndcg10, n_eval= evaluate_hit_ndcg(model, K=10)\n",
    "    print(f\"[Stage-2] Epoch {epoch+1} Train Loss BCE: {tr_loss:.4f} Users evaluated: {n_eval}, Hit@10: {hit10:.4f}, NDCG@10: {ndcg10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Save NCF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./checkpoints/ncf_finetuned.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"./checkpoints/ncf_finetuned.pt\")\n",
    "print(\"Model saved to ./checkpoints/ncf_finetuned.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train & Evaluate the Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse user_seen_items, test_user_to_item from above\n",
    "# if missing, rebuild from train_df, test_df\n",
    "# seen by user= defaultdict(set)\n",
    "# for u, m in zip(train_df[\"user_idx\"].values, train_df[\"movie_idx\"].values): seen_by_user[int(u)].add(int(m))\n",
    "# test_user_to_item= dict(zip(test_df[\"user_idx\"].astype(int).values, test_df[\"movie_idx\"].astype(int).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Utilities/ Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_ndcg_at_k(model_fn, test_user_to_item, K=10, batch_items=4096):\n",
    "    \"\"\" model_fn(user_idx, item_idx) -> scores(np.array) or torch.tensor\"\"\"\n",
    "    hits, ndcgs, total= 0, 0.0, 0\n",
    "    for user_idx, true_m_idx in test_user_to_item.items():\n",
    "\n",
    "        seen_items= user_seen_items.get(user_idx, set())\n",
    "\n",
    "        # candidates = all items excluding seen items\n",
    "        candidates= np.array([i for i in range(num_items) if i not in seen_items], dtype=np.int32)\n",
    "\n",
    "        if len(candidates) == 0 or true_m_idx not in candidates:\n",
    "            continue  # no candidates to rank or true item already seen (edge case)\n",
    "\n",
    "        scores= []\n",
    "\n",
    "        for i in range(0, len(candidates), batch_items):\n",
    "            batch = candidates[i:i + batch_items]\n",
    "            s= model_fn(user_idx, batch)  # np.array or torch.tensor\n",
    "            if isinstance(s, torch.Tensor):\n",
    "                s= s.detach().cpu().numpy()\n",
    "            scores.append(s)\n",
    "        scores= np.concatenate(scores, axis=0)  # [num_candidates]\n",
    "\n",
    "        # Rank candidates by score (higher is better)\n",
    "        ranked_indices= np.argsort(-scores)     # indices of candidates sorted by score desc\n",
    "        ranked_items= candidates[ranked_indices]  # Numpy array of ranked item indices\n",
    "\n",
    "        # 1-based rank of true item\n",
    "        pos= np.where(ranked_items == true_m_idx)[0]\n",
    "        if pos.size == 0:\n",
    "            continue\n",
    "        rank= int(pos[0]) + 1\n",
    "        total += 1\n",
    "        if rank <= K:\n",
    "            hits += 1\n",
    "            ndcgs += 1.0 / np.log2(rank + 1)\n",
    "\n",
    "    hitk= hits / max(1, total)\n",
    "    ndcgk= ndcgs / max(1, total)\n",
    "    return hitk, ndcgk, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId         int64\n",
      "movieId        int64\n",
      "rating       float64\n",
      "timestamp      int64\n",
      "user_idx       int64\n",
      "movie_idx      int32\n",
      "y            float32\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>movie_idx</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1172</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759205</td>\n",
       "      <td>0</td>\n",
       "      <td>1116</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2</td>\n",
       "      <td>405</td>\n",
       "      <td>2.0</td>\n",
       "      <td>835356246</td>\n",
       "      <td>1</td>\n",
       "      <td>398</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>3</td>\n",
       "      <td>736</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1298932787</td>\n",
       "      <td>2</td>\n",
       "      <td>711</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>4</td>\n",
       "      <td>2454</td>\n",
       "      <td>5.0</td>\n",
       "      <td>949982274</td>\n",
       "      <td>3</td>\n",
       "      <td>2320</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>5</td>\n",
       "      <td>4025</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1163375145</td>\n",
       "      <td>4</td>\n",
       "      <td>3877</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId  movieId  rating   timestamp  user_idx  movie_idx    y\n",
       "4         1     1172     4.0  1260759205         0       1116  1.0\n",
       "66        2      405     2.0   835356246         1        398  0.0\n",
       "110       3      736     3.5  1298932787         2        711  0.0\n",
       "304       4     2454     5.0   949982274         3       2320  1.0\n",
       "414       5     4025     4.5  1163375145         4       3877  1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_df.dtypes)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def rmse_mae_on_test(model_pred_fn, test_df):\n",
    "    \"\"\" model_pred_fn(user_idx, movie_idx) -> predicted scores(np.array) or torch.tensor\"\"\"\n",
    "    user= test_df[\"user_idx\"].values.astype(\"int32\")\n",
    "    movie= test_df[\"movie_idx\"].values.astype(\"int32\")\n",
    "    y_true= test_df[\"y\"].values.astype(\"float32\") # y =rating\n",
    "    preds= []\n",
    "    B=4096\n",
    "    for i in range(0, len(user), B):\n",
    "        batch_user= user[i:i+B]\n",
    "        batch_movie= movie[i:i+B]\n",
    "        p= model_pred_fn(batch_user, batch_movie)  # np.array or torch.tensor\n",
    "        if isinstance(p, torch.Tensor):\n",
    "            p= p.detach().cpu().numpy()\n",
    "        preds.append(p)\n",
    "    y_pred= np.concatenate(preds)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred)), mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Content-only scorer (fast dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content score between user and candidate items\n",
    "# approach 1 (user profile= mean of seen item embeddings of user)\n",
    "user_profile_cache= {}\n",
    "def build_user_profile_mean(user_idx):\n",
    "    seen = list(user_seen_items.get(user_idx, []))\n",
    "    if not seen:\n",
    "        return None\n",
    "    vec = movie_embeddings[seen].mean(axis=0)\n",
    "    vec = vec/np.linalg.norm(vec) if np.linalg.norm(vec)>0 else vec # L2-normalize\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_score_user(user_idx, item_idx):\n",
    "    \"\"\" Return array of cos-sim scores for item_indxs\"\"\"\n",
    "    profile= user_profile_cache.get(user_idx)\n",
    "    if profile is None:\n",
    "        profile= build_user_profile_mean(user_idx)\n",
    "        user_profile_cache[user_idx]= profile\n",
    "    if profile is None:\n",
    "        # cold users: return popularity or zeros\n",
    "        return np.zeros(len(item_idx), dtype=\"float32\")  # no profile, return zeros\n",
    "    return movie_embeddings[item_idx] @ profile  # dot products (cos sim since L2-normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Collaborative scorer (NCF probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def ncf_score_user(user_idx, item_idx):\n",
    "    # batches\n",
    "    out=[]\n",
    "    user_tensor= torch.tensor([user_idx], dtype=torch.long, device=device)\n",
    "    for i in range(0, len(item_idx), 4096):\n",
    "        batch= item_idx[i:i+4096]\n",
    "        batch_user= user_tensor.repeat(len(batch))\n",
    "        batch_item= torch.tensor(batch, dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits= model(batch_user, batch_item)\n",
    "        out.append(logits.detach().cpu().numpy())\n",
    "    return np.concatenate(out).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Hybrid scorer & alpha tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_score_user(user_idx, item_idx, alpha):\n",
    "    \"\"\" alpha=0.0 means pure content, alpha=1.0 means pure NCF\"\"\"\n",
    "    c= content_score_user(user_idx, item_idx).astype(np.float32)\n",
    "    n= ncf_score_user(user_idx, item_idx).astype(np.float32)\n",
    "\n",
    "    # normalize each vector\n",
    "    def znorm(x):\n",
    "        if x.std()==0:\n",
    "            return x - x.mean()\n",
    "        return (x - x.mean())/(x.std()+1e-9)\n",
    "    \n",
    "    c2= znorm(c)\n",
    "    n2= znorm(n)\n",
    "\n",
    "    return alpha * c2 + (1.0-alpha) * n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.00 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "Alpha 0.10 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "Alpha 0.20 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "Alpha 0.30 Users evaluated: 671, Hit@10: 0.0015, NDCG@10: 0.0004\n",
      "Alpha 0.40 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "Alpha 0.50 Users evaluated: 671, Hit@10: 0.0045, NDCG@10: 0.0014\n",
      "Alpha 0.60 Users evaluated: 671, Hit@10: 0.0060, NDCG@10: 0.0022\n",
      "Alpha 0.70 Users evaluated: 671, Hit@10: 0.0060, NDCG@10: 0.0024\n",
      "Alpha 0.80 Users evaluated: 671, Hit@10: 0.0075, NDCG@10: 0.0028\n",
      "Alpha 0.90 Users evaluated: 671, Hit@10: 0.0045, NDCG@10: 0.0016\n",
      "Alpha 1.00 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "Best alpha by Hit@10: (0.8, 0.007451564828614009, 0.0028080878836385615, 671)\n",
      "Best alpha by NDCG@10: (0.8, 0.007451564828614009, 0.0028080878836385615, 671)\n"
     ]
    }
   ],
   "source": [
    "# wrapper for evaluator that uses hybrid_score_user\n",
    "def make_hybrid_fn(alpha):\n",
    "    def fn(user_idx, item_idx):\n",
    "        return hybrid_score_user(user_idx, item_idx, alpha)\n",
    "    return fn\n",
    "\n",
    "# Grid search over alpha\n",
    "alphas= np.linspace(0,1,11) # 0.0 .... 1.0\n",
    "results= []\n",
    "for alpha in alphas:\n",
    "    model_fn= make_hybrid_fn(alpha)\n",
    "    hit10, ndcg10, n_eval= hit_ndcg_at_k(model_fn, test_user_to_item, K=10)\n",
    "    results.append( (alpha, hit10, ndcg10, n_eval) )\n",
    "    print(f\"Alpha {alpha:.2f} Users evaluated: {n_eval}, Hit@10: {hit10:.4f}, NDCG@10: {ndcg10:.4f}\")\n",
    "\n",
    "# Pick best by Hit or NDCG\n",
    "best_by_hit= max(results, key=lambda x: x[1])\n",
    "best_by_ndcg= max(results, key=lambda x: x[2])\n",
    "print(\"Best alpha by Hit@10:\", best_by_hit)  \n",
    "print(\"Best alpha by NDCG@10:\", best_by_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Offline ranking metrics (Hit@K / NDCG@K) & rating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF RMSE: 2.7981, MAE: 2.5926\n"
     ]
    }
   ],
   "source": [
    "import scipy.special\n",
    "\n",
    "# For BCE trained modelm get probability via sigmoid\n",
    "def ncf_prob_batch(user_idx, movie_idx):\n",
    "    user_tensor= torch.tensor(user_idx, dtype=torch.long, device=device)\n",
    "    movie_tensor= torch.tensor(movie_idx, dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        logits= model(user_tensor, movie_tensor).detach().cpu().numpy()\n",
    "    probs= scipy.special.expit(logits)  # sigmoid\n",
    "    # map prob-> rating e.g. rating= 1 + 4*prob\n",
    "    return 1.0 + 4.0 * probs\n",
    "\n",
    "# Compute RMSE/MAE of NCF on test set\n",
    "rmse, mae= rmse_mae_on_test(ncf_prob_batch, test_df)\n",
    "print(f\"NCF RMSE: {rmse:.4f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Stage-2 fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2b] Epoch 1 Train Loss BCE: 0.4043 Users evaluated: 671, Hit@10: 0.0015, NDCG@10: 0.0004\n",
      "[Stage-2b] Epoch 2 Train Loss BCE: 0.3506 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n",
      "[Stage-2b] Epoch 3 Train Loss BCE: 0.2948 Users evaluated: 671, Hit@10: 0.0000, NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model.item_emb.weight.requires_grad = True\n",
    "opt= torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)  # lower LR for fine-tuning\n",
    "for epoch in range(3):\n",
    "    train_loss= train_one_epoch(model, trainloader, opt, loss_fn)\n",
    "    hit10, ndcg10, n_eval= evaluate_hit_ndcg(model, K=10)\n",
    "    print(f\"[Stage-2b] Epoch {epoch+1} Train Loss BCE: {train_loss:.4f} Users evaluated: {n_eval}, Hit@10: {hit10:.4f}, NDCG@10: {ndcg10:.4f}\")\n",
    "\n",
    "np.savez_compressed(\"./features_artifacts/movie_embeddings_finetuned.npz\", tmdb_id=tmdb_ids, emb=movie_embeddings.astype(\"float32\"))\n",
    "torch.save(model.state_dict(), \"./checkpoints/ncf_finetuned_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.7 Export final artifacts for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu (or faiss-gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44383, 128)\n",
      "0.02 GB\n"
     ]
    }
   ],
   "source": [
    "print(movie_embeddings.shape)\n",
    "print(f\"{movie_embeddings.nbytes/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)\n",
    "import faiss\n",
    "print(faiss.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS works!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(\"FAISS works!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors: 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "x = np.random.rand(1000, 128).astype('float32')\n",
    "faiss.normalize_L2(x)\n",
    "index = faiss.IndexFlatIP(128)\n",
    "index.add(x)\n",
    "print(\"Total vectors:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors: 44383\n",
      "Saved FAISS flat index âœ…\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, faiss\n",
    "\n",
    "# 1) Start from your original matrix (whatever it is named)\n",
    "X = movie_embeddings  # shape (44383, 128)\n",
    "\n",
    "# 2) Ensure float32, materialize a brand-new C-contiguous, writeable buffer\n",
    "X = np.asarray(X, dtype=np.float32, order='C').copy()\n",
    "\n",
    "# 3) Clean bad rows\n",
    "#    - Replace NaN/Inf with 0\n",
    "#    - Remove/repair zero-norm rows to avoid division by zero\n",
    "bad = ~np.isfinite(X)\n",
    "if bad.any():\n",
    "    X[bad] = 0.0\n",
    "\n",
    "# 4) Safe L2 normalization in NumPy (avoid faiss.normalize_L2 on macOS ARM)\n",
    "norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "zero_mask = norms.squeeze(-1) == 0.0\n",
    "# set true zero rows to a tiny vector so norm isn't zero\n",
    "X[zero_mask] = 0.0\n",
    "norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "norms[norms == 0.0] = 1.0\n",
    "X = X / norms\n",
    "\n",
    "# 5) Build a simple flat IP index and add vectors\n",
    "d = X.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(X)               # should be instant for 44k x 128\n",
    "print(\"Total vectors:\", index.ntotal)  # expect 44383\n",
    "\n",
    "# 6) Save with a neutral extension (NOT .ivf for a flat index)\n",
    "faiss.write_index(index, \"./features_artifacts/movie_faiss_index_flat.index\")\n",
    "print(\"Saved FAISS flat index âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel crash:\n",
    "# import numpy as np, faiss\n",
    "\n",
    "# # Suppose your matrix is named `movie_embeddings`\n",
    "# arr = np.require(movie_embeddings, dtype=np.float32, requirements=['C','W','A'])\n",
    "# print(arr.shape, arr.dtype, arr.flags['C_CONTIGUOUS'], arr.flags['WRITEABLE'])\n",
    "\n",
    "# faiss.normalize_L2(arr)  # in-place normalize; needs C-contiguous & writeable\n",
    "# idx = faiss.IndexFlatIP(arr.shape[1])\n",
    "# idx.add(arr)\n",
    "# print(\"OK real:\", idx.ntotal)  # expect 44383\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel crash:\n",
    "# import faiss\n",
    "# d = movie_embeddings.shape[1]\n",
    "# faiss.normalize_L2(movie_embeddings)\n",
    "# index = faiss.IndexFlatIP(d)\n",
    "# index.add(movie_embeddings.astype(\"float32\"))\n",
    "# faiss.write_index(index, \"./features_artifacts/movie_faiss_index_flat.index\")\n",
    "# print(\"FAISS flat index saved âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cold start handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Cold movies(no ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed movie TMDB ID: 862\n"
     ]
    }
   ],
   "source": [
    "print(\"Seed movie TMDB ID:\", tmdb_ids[seed_idx])\n",
    "# https://www.themoviedb.org/movie/862-toy-story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors of TMDB ID 862 (Toy Story) by content only: [   863    771 196633   9969  37556  44157  31309  16508  36251  73541] Scores: [0.9179871  0.8985822  0.89654386 0.8953173  0.89306736 0.88762194\n",
      " 0.8855413  0.88159513 0.8811209  0.8805306 ]\n"
     ]
    }
   ],
   "source": [
    "# Ensure movie_embeddings rows are L2-normalized\n",
    "def topk_by_content_for_movie(movie_row_idx, k=20):\n",
    "    query_vec= movie_embeddings[movie_row_idx] # normalized\n",
    "    sims= movie_embeddings @ query_vec  # cosine similarity (dot product)\n",
    "    sims[movie_row_idx] = -1.0          # exclude self\n",
    "    nbrs= np.argpartition(-sims, k)[:k]\n",
    "    nbrs= nbrs[np.argsort(-sims[nbrs])]\n",
    "    return nbrs, sims[nbrs]\n",
    "\n",
    "# Example if movie 0 is â€œRush Hourâ€ (Action, Comedy), positives will be any other Action or Comedy movies.\n",
    "idx, scores= topk_by_content_for_movie(seed_idx, k=10)\n",
    "print(\"Neighbors of TMDB ID 862 (Toy Story) by content only:\", tmdb_ids[idx], \"Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_popular_items(k=20):\n",
    "    stats= ratings_cleaned.groupby(\"movie_idx\")[\"rating\"].agg([\"count\",\"mean\"])\n",
    "    # weighted score= count* mean_rating\n",
    "    stats[\"weighted\"]= stats[\"count\"] * stats[\"mean\"]\n",
    "    topk= stats.sort_values(\"weighted\", ascending=False).head(k).index.tolist()\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_profile_mean(user_idx):\n",
    "    seen = list(user_seen_items.get(user_idx, []))\n",
    "    if not seen:\n",
    "        return None\n",
    "    vec = movie_embeddings[seen].mean(axis=0)\n",
    "    norm= np.linalg.norm(vec)\n",
    "    if norm==0:\n",
    "        return vec\n",
    "    return vec/norm # L2-normalize\n",
    "\n",
    "def recommend_content_for_user(user_idx, k=20):\n",
    "    profile= build_user_profile_mean(user_idx)\n",
    "    if profile is None:\n",
    "        # cold user fallback- popularity or random\n",
    "        return top_popular_items(k)\n",
    "    \n",
    "    sims= movie_embeddings @ profile  # cosine similarity (dot product)\n",
    "    # filter seen items\n",
    "    seen_items= user_seen_items.get(user_idx, set())\n",
    "    sims_list= [(s, i) for i, s in enumerate(sims) if i not in seen_items]\n",
    "    sims_list.sort(key=lambda x: -x[0])  # sort by score desc\n",
    "    topk= sims_list[:k]\n",
    "    return [i for _, i in topk], [s for s, _ in topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Cold movies(few/no ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_by_num_ratings(n_ratings, C=10.0):\n",
    "    # when n_ratings=0 -> alpha=1.0 (content-only)\n",
    "    # as n_ratings increases, alpha -> 0 (collab dominates)\n",
    "    return float(min(1.0, C / (C + n_ratings)))\n",
    "\n",
    "def ncf_score_user_array(user_idx, item_idx, batch_size=4096):\n",
    "    \"\"\" Return array of scores for item_indxs\"\"\"\n",
    "    model.eval()\n",
    "    user_tensor= torch.tensor([user_idx], dtype=torch.long, device=device)\n",
    "    out=[]\n",
    "    for i in range(0, len(item_idx), batch_size):\n",
    "        batch= item_idx[i:i+batch_size]\n",
    "        batch_user= user_tensor.repeat(len(batch))\n",
    "        batch_item= torch.tensor(batch, dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits= model(batch_user, batch_item)\n",
    "        out.append(logits.detach().cpu().numpy())\n",
    "    return np.concatenate(out).astype(\"float32\") # shape = (len(item_idxs),)\n",
    "\n",
    "def zscore(x):\n",
    "    if x.std()==0:\n",
    "        return x - x.mean()\n",
    "    return (x - x.mean())/(x.std()+1e-9)\n",
    "\n",
    "def hybrid_for_user(user_idx, item_idx, C=10.0):\n",
    "    # item_idx = list/np.array of candidate item indices to score\n",
    "    # compute content\n",
    "    profile= build_user_profile_mean(user_idx)\n",
    "    if profile is None:\n",
    "        c_scores= np.zeros(len(item_idx), dtype=\"float32\")  # no profile, return zeros\n",
    "    else:\n",
    "        c_scores= movie_embeddings[item_idx] @ profile  # dot products (cos sim since L2-normalized)\n",
    "\n",
    "    # collaborative\n",
    "    cf_scores= ncf_score_user_array(user_idx, item_idx)\n",
    "\n",
    "    # per user normalization\n",
    "    c2= zscore(c_scores)\n",
    "    cf2= zscore(cf_scores)\n",
    "\n",
    "    # compute alpha from user's training count\n",
    "    n_ratings= len(user_seen_items.get(user_idx, []))\n",
    "    alpha= alpha_by_num_ratings(n_ratings, C=C) # alpha=1.0 means pure content, alpha=0.0 means pure collab\n",
    "    scores= alpha * c2 + (1.0 - alpha) * cf2\n",
    "    return scores, alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 recommendations for user 1 : [  9555  10149   5769 183894  12501   1908  25237 133919  17015  14580\n",
      "  17447 149870  11040  17961  84329  24019  20540  12717  16233 234200] Alpha: 0.3448275862068966\n"
     ]
    }
   ],
   "source": [
    "# pick an external MovieLens userId (e.g., 1)\n",
    "user_id = 1\n",
    "u_idx = u2i[user_id]                      # dict lookup â†’ integer index\n",
    "\n",
    "seen = user_seen_items.get(u_idx, set())     # set of item indices this user has seen\n",
    "\n",
    "candidates = np.array(\n",
    "    [i for i in range(movie_embeddings.shape[0]) if i not in seen],\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "scores, alpha = hybrid_for_user(u_idx, candidates, C=10.0)\n",
    "\n",
    "order = np.argsort(-scores)[:20]\n",
    "top_items = candidates[order]\n",
    "\n",
    "print(\"Top 20 recommendations for user\", user_id, \":\", tmdb_ids[top_items], \"Alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Cold movies(few/no ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7C: Hybrid Fallback Policy (matches your variable names)\n",
    "\n",
    "# simple alpha schedule for cold users\n",
    "def alpha_by_num_ratings(n_ratings, C=10.0):\n",
    "    return float(min(1.0, C / (C + max(0, n_ratings))))\n",
    "\n",
    "# user profile from content side (mean of seen item embeddings)\n",
    "def build_user_profile_mean(u_idx):\n",
    "    seen = list(user_seen_items.get(u_idx, set()))\n",
    "    if not seen:\n",
    "        return None\n",
    "    prof = movie_embeddings[seen].mean(axis=0)\n",
    "    norm = np.linalg.norm(prof)\n",
    "    if norm == 0:\n",
    "        return None\n",
    "    return prof / norm\n",
    "\n",
    "# content-only recommenders\n",
    "def recommend_content_for_user(u_idx, k=20):\n",
    "    prof = build_user_profile_mean(u_idx)\n",
    "    if prof is None:\n",
    "        # true cold user: fallback to popularity\n",
    "        return np.array(top_popular_items(k), dtype=np.int32), None\n",
    "    sims = movie_embeddings @ prof\n",
    "    seen = user_seen_items.get(u_idx, set())\n",
    "    cand = [(i, sims[i]) for i in range(len(movie_embeddings)) if i not in seen]\n",
    "    cand.sort(key=lambda x: -x[1])\n",
    "    top = [i for i,_ in cand[:k]]\n",
    "    return np.array(top, dtype=np.int32), None\n",
    "\n",
    "def recommend_neighbors_by_seed_tmdb(seed_tmdb_id, k=20):\n",
    "    # content neighbors for a given seed movie\n",
    "    # assumes L2-normalized rows in movie_embeddings\n",
    "    row = int(np.where(tmdb_ids == seed_tmdb_id)[0][0])\n",
    "    q = movie_embeddings[row]\n",
    "    sims = movie_embeddings @ q\n",
    "    sims[row] = -1.0\n",
    "    idx = np.argpartition(-sims, k)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return idx, sims[idx]\n",
    "\n",
    "# main entry: hybrid with fallbacks\n",
    "def recommend_hybrid(user_id, k=20, C=10.0, seed_tmdb_id=None):\n",
    "    \"\"\"\n",
    "    Policy:\n",
    "      - If seed_tmdb_id is provided -> content neighbors of that seed (movieâ†’movie).\n",
    "      - Else if user has no history -> popularity (or content from a cold profile if available).\n",
    "      - Else -> hybrid blend with alpha based on user's history length.\n",
    "    Returns: (top_item_indices, scores, alpha_used)\n",
    "    \"\"\"\n",
    "    # seed path (movie-to-movie)\n",
    "    if seed_tmdb_id is not None:\n",
    "        idxs, _ = recommend_neighbors_by_seed_tmdb(seed_tmdb_id, k=k)\n",
    "        return idxs, None, 1.0  # content-only, alpha conceptually 1.0\n",
    "\n",
    "    # user path\n",
    "    if user_id not in u2i:\n",
    "        raise ValueError(f\"user_id {user_id} not in mapping\")\n",
    "    u_idx = u2i[user_id]\n",
    "    seen = user_seen_items.get(u_idx, set())\n",
    "\n",
    "    # cold user\n",
    "    if not seen:\n",
    "        top, _ = recommend_content_for_user(u_idx, k=k)\n",
    "        # if even profile is None, top is popularity list already\n",
    "        return top, None, 1.0\n",
    "\n",
    "    # hybrid user\n",
    "    candidates = np.array([i for i in range(movie_embeddings.shape[0]) if i not in seen], dtype=np.int32)\n",
    "    scores, alpha = hybrid_for_user(u_idx, candidates, C=C)  # uses your existing function\n",
    "    order = np.argsort(-scores)[:k]\n",
    "    top_items = candidates[order]\n",
    "    return top_items, scores[order], alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Tune C (alpha schedule) and evaluate on low-history users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1     Users=42    Hit@10=0.0000  NDCG@10=0.0000\n",
      "C=3     Users=42    Hit@10=0.0000  NDCG@10=0.0000\n",
      "C=5     Users=42    Hit@10=0.0000  NDCG@10=0.0000\n",
      "C=10    Users=42    Hit@10=0.0000  NDCG@10=0.0000\n",
      "C=20    Users=42    Hit@10=0.0000  NDCG@10=0.0000\n",
      "Best (by NDCG): (1, 0.0, 0.0, 42)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hit_ndcg_for_low_users(Cs=(1,3,5,10,20), K=10, max_users=None, max_seen=10):\n",
    "    \"\"\"\n",
    "    Evaluate hybrid_for_user on users with few training interactions (<= max_seen),\n",
    "    sweeping C values. Returns list of tuples: (C, Hit@K, NDCG@K, n_users).\n",
    "    \"\"\"\n",
    "    # select low-history users who also have a test item\n",
    "    low_users = [u for u in test_user_to_item.keys()\n",
    "                 if 0 < len(user_seen_items.get(u, set())) <= max_seen]\n",
    "\n",
    "    if max_users is not None:\n",
    "        low_users = low_users[:max_users]\n",
    "\n",
    "    results = []\n",
    "    for C in Cs:\n",
    "        hits = 0; ndcg = 0.0; total = 0\n",
    "        for u_idx in low_users:\n",
    "            seen = user_seen_items.get(u_idx, set())\n",
    "            # candidates = all items except seen\n",
    "            candidates = np.array([i for i in range(movie_embeddings.shape[0]) if i not in seen],\n",
    "                                  dtype=np.int32)\n",
    "            if len(candidates) == 0:\n",
    "                continue\n",
    "\n",
    "            scores, _alpha = hybrid_for_user(u_idx, candidates, C=C)\n",
    "            order = np.argsort(-scores)\n",
    "            ranked = candidates[order]\n",
    "            true_m = test_user_to_item[u_idx]\n",
    "            pos = np.where(ranked == true_m)[0]\n",
    "            if pos.size == 0:\n",
    "                continue\n",
    "\n",
    "            r = int(pos[0]) + 1\n",
    "            total += 1\n",
    "            if r <= K:\n",
    "                hits += 1\n",
    "                ndcg += 1.0 / np.log2(r + 1)\n",
    "\n",
    "        hitk  = hits / max(1, total)\n",
    "        ndcgk = ndcg / max(1, total)\n",
    "        results.append((C, hitk, ndcgk, total))\n",
    "        print(f\"C={C:<4}  Users={total:<4}  Hit@{K}={hitk:.4f}  NDCG@{K}={ndcgk:.4f}\")\n",
    "\n",
    "    best = max(results, key=lambda x: x[2]) if results else None\n",
    "    if best:\n",
    "        print(\"Best (by NDCG):\", best)\n",
    "    else:\n",
    "        print(\"No evaluable users found.\")\n",
    "    return results, best\n",
    "\n",
    "# Try with a slightly looser cutoff first\n",
    "_7d_results, _7d_best = hit_ndcg_for_low_users(Cs=(1,3,5,10,20), K=10, max_users=None, max_seen=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Low users] Content-only  Users=42  Hit@10=0.0000  NDCG@10=0.0000\n"
     ]
    }
   ],
   "source": [
    "def eval_content_only_low_users(K=10, max_users=None, max_seen=10):\n",
    "    hits = 0; ndcg = 0.0; total = 0\n",
    "    low_users = [u for u in test_user_to_item.keys()\n",
    "                 if 0 < len(user_seen_items.get(u, set())) <= max_seen]\n",
    "    if max_users: low_users = low_users[:max_users]\n",
    "\n",
    "    for u_idx in low_users:\n",
    "        seen = user_seen_items.get(u_idx, set())\n",
    "        prof = build_user_profile_mean(u_idx)\n",
    "        if prof is None:\n",
    "            continue\n",
    "        sims = movie_embeddings @ prof\n",
    "        candidates = np.array([i for i in range(movie_embeddings.shape[0]) if i not in seen], dtype=np.int32)\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        scores = sims[candidates]\n",
    "        order = np.argsort(-scores)\n",
    "        ranked = candidates[order]\n",
    "        true_m = test_user_to_item[u_idx]\n",
    "        pos = np.where(ranked == true_m)[0]\n",
    "        if pos.size == 0:\n",
    "            continue\n",
    "        r = int(pos[0]) + 1\n",
    "        total += 1\n",
    "        if r <= K:\n",
    "            hits += 1\n",
    "            ndcg += 1.0 / np.log2(r + 1)\n",
    "    return (hits / max(1,total), ndcg / max(1,total), total)\n",
    "\n",
    "content_hit, content_ndcg, content_n = eval_content_only_low_users(K=10, max_seen=20)\n",
    "print(f\"[Low users] Content-only  Users={content_n}  Hit@10={content_hit:.4f}  NDCG@10={content_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A) Prep: popularity vector (once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Popularity by item (MovieLens m_idx), min-max normalized to [0,1]\n",
    "# # Safe if you already computed something similar; redefining is harmless.\n",
    "# _pop = (\n",
    "#     ratings_df.groupby(\"movie_idx\")[\"rating\"].count()\n",
    "#     .reindex(range(movie_embeddings.shape[0]), fill_value=0)\n",
    "#     .astype(\"float32\")\n",
    "# ).values\n",
    "\n",
    "# def _minmax_vec(x):\n",
    "#     lo, hi = x.min(), x.max()\n",
    "#     return (x - lo) / (hi - lo + 1e-9) if hi > lo else np.zeros_like(x, dtype=np.float32)\n",
    "\n",
    "# popularity_norm = _minmax_vec(_pop)  # shape [N_items]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B) A min-max variant of your hybrid (optional tiny popularity prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you already have these helpers, this will just overwrite with identical behavior.\n",
    "# def alpha_by_num_ratings(n_ratings, C=10.0):\n",
    "#     return float(min(1.0, C / (C + max(0, n_ratings))))\n",
    "\n",
    "# def build_user_profile_mean(u_idx):\n",
    "#     seen = list(user_seen_items.get(u_idx, set()))\n",
    "#     if not seen:\n",
    "#         return None\n",
    "#     prof = movie_embeddings[seen].mean(axis=0)\n",
    "#     norm = np.linalg.norm(prof)\n",
    "#     if norm == 0:\n",
    "#         return None\n",
    "#     return prof / norm\n",
    "\n",
    "# # If you don't already have this defined, this one will suffice.\n",
    "# def ncf_score_user_array(u_idx, item_idxs, batch_size=4096):\n",
    "#     model.eval()\n",
    "#     out = []\n",
    "#     u_t = torch.tensor([u_idx], dtype=torch.long, device=device)\n",
    "#     for i in range(0, len(item_idxs), batch_size):\n",
    "#         batch = item_idxs[i:i+batch_size]\n",
    "#         uu = u_t.repeat(len(batch))\n",
    "#         mm = torch.tensor(batch, dtype=torch.long, device=device)\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(uu, mm).detach().cpu().numpy()   # higher = more likely\n",
    "#         out.append(logits.astype(np.float32))\n",
    "#     return np.concatenate(out, axis=0)\n",
    "\n",
    "# def hybrid_for_user_minmax(u_idx, item_idxs, C=3.0, pop_lambda=0.05):\n",
    "#     \"\"\"\n",
    "#     Same idea as your hybrid_for_user, but:\n",
    "#       - per-user MIN-MAX normalize content & collaborative scores\n",
    "#       - optional small popularity prior (already min-max global)\n",
    "#     Returns scores, alpha\n",
    "#     \"\"\"\n",
    "#     # content part\n",
    "#     prof = build_user_profile_mean(u_idx)\n",
    "#     if prof is None:\n",
    "#         c_scores = np.zeros(len(item_idxs), dtype=np.float32)\n",
    "#     else:\n",
    "#         c_scores = (movie_embeddings[item_idxs] @ prof).astype(np.float32)\n",
    "\n",
    "#     # collaborative part\n",
    "#     cf_scores = ncf_score_user_array(u_idx, item_idxs)\n",
    "\n",
    "#     # per-user normalization (min-max)\n",
    "#     def mm(x):\n",
    "#         lo, hi = x.min(), x.max()\n",
    "#         return (x - lo) / (hi - lo + 1e-9) if hi > lo else np.zeros_like(x, dtype=np.float32)\n",
    "\n",
    "#     c2  = mm(c_scores)\n",
    "#     cf2 = mm(cf_scores)\n",
    "\n",
    "#     # popularity prior on the same candidate set\n",
    "#     pop2 = popularity_norm[item_idxs]  # already [0,1]\n",
    "\n",
    "#     # alpha from userâ€™s history size\n",
    "#     n_ratings = len(user_seen_items.get(u_idx, set()))\n",
    "#     alpha = alpha_by_num_ratings(n_ratings, C=C)\n",
    "\n",
    "#     # blend\n",
    "#     scores = alpha * c2 + (1.0 - alpha) * cf2 + (pop_lambda * pop2)\n",
    "#     return scores.astype(np.float32), alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C) Quick compare on low-history users (K=10 and K=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_hybrid_variant_low_users(variant=\"zscore\", K=10, max_seen=10, C=3.0, pop_lambda=0.05):\n",
    "#     \"\"\"\n",
    "#     variant: \"zscore\" -> uses your existing hybrid_for_user(u, items, C)\n",
    "#              \"minmax\" -> uses hybrid_for_user_minmax(u, items, C, pop_lambda)\n",
    "#     \"\"\"\n",
    "#     users = [u for u in test_user_to_item.keys() if 0 < len(user_seen_items.get(u, set())) <= max_seen]\n",
    "#     hits = 0; ndcg = 0.0; total = 0\n",
    "#     for u_idx in users:\n",
    "#         seen = user_seen_items.get(u_idx, set())\n",
    "#         candidates = np.array([i for i in range(movie_embeddings.shape[0]) if i not in seen], dtype=np.int32)\n",
    "#         if len(candidates) == 0:\n",
    "#             continue\n",
    "#         if variant == \"minmax\":\n",
    "#             scores, _ = hybrid_for_user_minmax(u_idx, candidates, C=C, pop_lambda=pop_lambda)\n",
    "#         else:\n",
    "#             scores, _ = hybrid_for_user(u_idx, candidates, C=C)  # your existing implementation\n",
    "#         order = np.argsort(-scores)\n",
    "#         ranked = candidates[order]\n",
    "#         true_m = test_user_to_item[u_idx]\n",
    "#         pos = np.where(ranked == true_m)[0]\n",
    "#         if pos.size == 0:\n",
    "#             continue\n",
    "#         r = int(pos[0]) + 1\n",
    "#         total += 1\n",
    "#         if r <= K:\n",
    "#             hits += 1\n",
    "#             ndcg += 1.0 / np.log2(r + 1)\n",
    "#     return (hits / max(1,total), ndcg / max(1,total), total)\n",
    "\n",
    "# # Run comparisons\n",
    "# for K in (10, 20):\n",
    "#     z_hit, z_ndcg, z_n = eval_hybrid_variant_low_users(\"zscore\", K=K, max_seen=10, C=3.0)\n",
    "#     mm_hit, mm_ndcg, mm_n = eval_hybrid_variant_low_users(\"minmax\", K=K, max_seen=10, C=3.0, pop_lambda=0.05)\n",
    "#     print(f\"[Low users] K={K}  z-score: Users={z_n} Hit@{K}={z_hit:.4f} NDCG@{K}={z_ndcg:.4f}  \"\n",
    "#           f\"min-max+pop: Users={mm_n} Hit@{K}={mm_hit:.4f} NDCG@{K}={mm_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 1) Rebuild mappings (train â†’ seen, test â†’ held-out), minimal & consistent\n",
    "# user_seen_items = {}\n",
    "# for u, m in zip(train_df[\"user_idx\"].astype(int).values, train_df[\"movie_idx\"].astype(int).values):\n",
    "#     user_seen_items.setdefault(u, set()).add(m)\n",
    "\n",
    "# test_user_to_item = dict(\n",
    "#     zip(test_df[\"user_idx\"].astype(int).values, test_df[\"movie_idx\"].astype(int).values)\n",
    "# )\n",
    "\n",
    "# # 2) Quick diagnostics\n",
    "# users_with_test = list(test_user_to_item.keys())\n",
    "# low_users_10 = [u for u in users_with_test if 0 < len(user_seen_items.get(u, set())) <= 10]\n",
    "# low_users_20 = [u for u in users_with_test if 0 < len(user_seen_items.get(u, set())) <= 20]\n",
    "\n",
    "# print(f\"Users with test: {len(users_with_test)} | low<=10: {len(low_users_10)} | low<=20: {len(low_users_20)}\")\n",
    "\n",
    "# # Count problematic users where the true test item is somehow already in 'seen' (should be 0 for leave-last-1)\n",
    "# problem = sum(1 for u in users_with_test if test_user_to_item[u] in user_seen_items.get(u, set()))\n",
    "# print(\"Users with true test item mistakenly in 'seen':\", problem)\n",
    "\n",
    "# # 3) Minimal evaluator that uses YOUR hybrid_for_user and fixes only when necessary\n",
    "# def eval_hybrid_existing(K=10, max_seen=10, C=3.0):\n",
    "#     users = [u for u in users_with_test if 0 < len(user_seen_items.get(u, set())) <= max_seen]\n",
    "#     hits = 0; ndcg = 0.0; total = 0\n",
    "#     for u_idx in users:\n",
    "#         seen = user_seen_items.get(u_idx, set())\n",
    "#         true_m = test_user_to_item[u_idx]\n",
    "\n",
    "#         # candidates = all items except seen; ensure the true item is present\n",
    "#         candidates = np.array([i for i in range(movie_embeddings.shape[0]) if i not in seen], dtype=np.int32)\n",
    "#         if candidates.size == 0:\n",
    "#             continue\n",
    "#         if true_m not in candidates:\n",
    "#             # If this happens due to a prev split glitch, include it so we can still evaluate.\n",
    "#             candidates = np.append(candidates, true_m)\n",
    "\n",
    "#         scores, _alpha = hybrid_for_user(u_idx, candidates, C=C)\n",
    "#         order = np.argsort(-scores)\n",
    "#         ranked = candidates[order]\n",
    "#         pos = np.where(ranked == true_m)[0]\n",
    "#         if pos.size == 0:\n",
    "#             continue\n",
    "#         r = int(pos[0]) + 1\n",
    "#         total += 1\n",
    "#         if r <= K:\n",
    "#             hits += 1\n",
    "#             ndcg += 1.0 / np.log2(r + 1)\n",
    "\n",
    "#     hitk = hits / max(1, total)\n",
    "#     ndcgk = ndcg / max(1, total)\n",
    "#     return hitk, ndcgk, total\n",
    "\n",
    "# # 4) Run with your existing hybrid_for_user (no extra functions), K=10 and K=20\n",
    "# for K in (10, 20):\n",
    "#     hit, ndcg, n = eval_hybrid_existing(K=K, max_seen=10, C=3.0)\n",
    "#     print(f\"[Low users<=10] K={K}: Users={n}  Hit@{K}={hit:.4f}  NDCG@{K}={ndcg:.4f}\")\n",
    "\n",
    "# # If the <=10 slice is tiny, also show <=20 quickly:\n",
    "# for K in (10, 20):\n",
    "#     hit, ndcg, n = eval_hybrid_existing(K=K, max_seen=20, C=3.0)\n",
    "#     print(f\"[Low users<=20] K={K}: Users={n}  Hit@{K}={hit:.4f}  NDCG@{K}={ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5: Serving: FAISS shortlist â†’ hybrid re-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_HISTORY_MAX_SEEN = 20\n",
    "C_FOR_LOW_USERS = 3.0\n",
    "TOPK_TO_RETURN = 20  # or 10 if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (44383, 128) dtype: float32 contiguous: True finite: True\n"
     ]
    }
   ],
   "source": [
    "print(\"shape:\", movie_embeddings.shape,\n",
    "      \"dtype:\", movie_embeddings.dtype,\n",
    "      \"contiguous:\", movie_embeddings.flags['C_CONTIGUOUS'],\n",
    "      \"finite:\", np.isfinite(movie_embeddings).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index ready. Vectors: 44383 Dim: 128\n",
      "User 1 â†’ TMDB: [ 27022  31606  39314   9662  27205  41800     98 118490  11712  10856\n",
      " 266639 339967 128048 135397   1406  27957  31501   2926  57706  21332] alpha: 0.13636363636363635\n",
      "Neighbors of 862 â†’ TMDB: [   863    771 196633   9969  37556  44157  31309  16508  36251  73541]\n"
     ]
    }
   ],
   "source": [
    "# ---- 7E (robust): FAISS shortlist + hybrid re-rank with safe normalization ----\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def _safe_l2_normalize_rows(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    # Ensure float32, C-contiguous, finite, no zero norms\n",
    "    mat = np.asarray(mat, dtype=np.float32, order=\"C\")\n",
    "    np.nan_to_num(mat, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    norms = np.maximum(norms, eps)\n",
    "    mat = mat / norms\n",
    "    return mat\n",
    "\n",
    "def _safe_l2_normalize_vec(v: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=np.float32).reshape(1, -1)\n",
    "    np.nan_to_num(v, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    n = np.linalg.norm(v, axis=1, keepdims=True)\n",
    "    n = np.maximum(n, eps)\n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "# 0) Prepare embeddings safely\n",
    "movie_embeddings = _safe_l2_normalize_rows(movie_embeddings)\n",
    "tmdb_ids = np.asarray(tmdb_ids)  # ensure np array\n",
    "\n",
    "# 1) Build FAISS IP index (cosine if rows are L2-normalized)\n",
    "d = movie_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(d)\n",
    "faiss_index.add(movie_embeddings.astype(\"float32\", copy=False))\n",
    "print(\"FAISS index ready. Vectors:\", faiss_index.ntotal, \"Dim:\", d)\n",
    "\n",
    "# 2) Shortlist helpers\n",
    "def shortlist_by_content_for_user(u_idx, M=200):\n",
    "    \"\"\"\n",
    "    Use the user's content profile to pull a shortlist via FAISS.\n",
    "    Falls back to simple unseen-scan if the user is totally cold.\n",
    "    \"\"\"\n",
    "    prof = build_user_profile_mean(u_idx)\n",
    "    seen = user_seen_items.get(u_idx, set())\n",
    "\n",
    "    if prof is None:\n",
    "        # fallback: first M unseen (deterministic & safe)\n",
    "        if len(seen) >= movie_embeddings.shape[0]:\n",
    "            return np.array([], dtype=np.int32)\n",
    "        idxs = [i for i in range(movie_embeddings.shape[0]) if i not in seen][:M]\n",
    "        return np.array(idxs, dtype=np.int32)\n",
    "\n",
    "    q = _safe_l2_normalize_vec(prof)  # (1, d) float32 normalized\n",
    "    sims, idxs = faiss_index.search(q, M)  # idxs: (1, M)\n",
    "    idxs = idxs[0]\n",
    "    # filter out seen\n",
    "    if len(seen) > 0:\n",
    "        idxs = np.array([i for i in idxs if i not in seen], dtype=np.int32)\n",
    "    else:\n",
    "        idxs = idxs.astype(np.int32, copy=False)\n",
    "    return idxs\n",
    "\n",
    "def shortlist_by_content_seed(seed_tmdb_id, M=200):\n",
    "    \"\"\"\n",
    "    Movieâ†’movie: FAISS neighbors of a seed movie (content-only).\n",
    "    \"\"\"\n",
    "    seed_pos = np.where(tmdb_ids == seed_tmdb_id)[0]\n",
    "    if seed_pos.size == 0:\n",
    "        raise ValueError(f\"Seed TMDB id {seed_tmdb_id} not found.\")\n",
    "    row = int(seed_pos[0])\n",
    "\n",
    "    q = _safe_l2_normalize_vec(movie_embeddings[row])\n",
    "    sims, idxs = faiss_index.search(q, M)\n",
    "    idxs = idxs[0]\n",
    "    # remove self if present\n",
    "    idxs = idxs[idxs != row]\n",
    "    return idxs.astype(np.int32, copy=False)\n",
    "\n",
    "# 3) Main entry: shortlist â†’ hybrid re-rank\n",
    "def recommend_hybrid_with_faiss(user_id, k=TOPK_TO_RETURN, M=200, C=None, seed_tmdb_id=None):\n",
    "    \"\"\"\n",
    "    Two-stage serving:\n",
    "      1) FAISS shortlist by content (user profile OR seed movie)\n",
    "      2) Hybrid re-rank the shortlist with your existing hybrid_for_user (z-score)\n",
    "    Returns: (top_item_indices, scores_or_None, alpha_used)\n",
    "    \"\"\"\n",
    "    if C is None:\n",
    "        C = C_FOR_LOW_USERS\n",
    "\n",
    "    # Seed path (movie-to-movie content recommendations)\n",
    "    if seed_tmdb_id is not None:\n",
    "        shortlist = shortlist_by_content_seed(seed_tmdb_id, M=M)\n",
    "        return shortlist[:k], None, 1.0\n",
    "\n",
    "    # User path\n",
    "    if user_id not in u2i:\n",
    "        raise ValueError(f\"user_id {user_id} not in mapping\")\n",
    "    u_idx = u2i[user_id]\n",
    "\n",
    "    shortlist = shortlist_by_content_for_user(u_idx, M=M)\n",
    "    if shortlist.size == 0:\n",
    "        return np.array([], dtype=np.int32), None, 1.0\n",
    "\n",
    "    # hybrid re-rank on shortlist\n",
    "    scores, alpha = hybrid_for_user(u_idx, shortlist, C=C)\n",
    "    order = np.argsort(-scores)[:k]\n",
    "    topk = shortlist[order]\n",
    "    return topk, scores[order], alpha\n",
    "\n",
    "# 4) Examples\n",
    "# A) user recommendations (hybrid)\n",
    "topk, scores, alpha = recommend_hybrid_with_faiss(user_id=1, k=TOPK_TO_RETURN, M=200, C=C_FOR_LOW_USERS)\n",
    "print(\"User 1 â†’ TMDB:\", tmdb_ids[topk], \"alpha:\", alpha)\n",
    "\n",
    "# B) movieâ†’movie neighbors (content-only)\n",
    "neighbors, _, _ = recommend_hybrid_with_faiss(user_id=1, seed_tmdb_id=862, k=10, M=200)\n",
    "print(\"Neighbors of 862 â†’ TMDB:\", tmdb_ids[neighbors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote u2i.json (671 users), user_seen_items.pkl (671 users), id_to_title.json (44383 titles).\n",
      "Sample user IDs: [1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# === Export mappings for the API (run after Step 3 & 4) ===\n",
    "# Requires in memory:\n",
    "# - train_df (from your Step 4 split, with user/item indices)\n",
    "# - movie_embeddings, tmdb_ids (from your Step 3/6 content tower)\n",
    "# - a movies-like DataFrame with columns ['id','title'] (e.g., metadata_cleaned or movies)\n",
    "\n",
    "import os, json, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"./mappings\", exist_ok=True)\n",
    "\n",
    "# ---- 1) Find the right column names in train_df ----\n",
    "def _pick(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "if 'train_df' not in globals():\n",
    "    raise RuntimeError(\"train_df not found. Run your Step 4 split first.\")\n",
    "\n",
    "user_ext_col = _pick(train_df, (\"userId\", \"user_id\", \"userid\"))\n",
    "user_idx_col = _pick(train_df, (\"u_idx\", \"user_idx\", \"user_index\"))\n",
    "movie_idx_col = _pick(train_df, (\"m_idx\", \"movie_idx\", \"item_idx\", \"movie_index\"))\n",
    "\n",
    "if not all([user_ext_col, user_idx_col, movie_idx_col]):\n",
    "    raise RuntimeError(\n",
    "        f\"Could not find needed columns in train_df. \"\n",
    "        f\"Found user_ext_col={user_ext_col}, user_idx_col={user_idx_col}, movie_idx_col={movie_idx_col}. \"\n",
    "        f\"Open train_df.head() to confirm column names.\"\n",
    "    )\n",
    "\n",
    "# ---- 2) Build u2i: external MovieLens userId -> internal user index (u_idx) ----\n",
    "tmp = train_df[[user_ext_col, user_idx_col]].drop_duplicates()\n",
    "u2i = {int(r[user_ext_col]): int(r[user_idx_col]) for _, r in tmp.iterrows()}\n",
    "\n",
    "# ---- 3) Build user_seen_items from TRAIN: u_idx -> set of m_idx ----\n",
    "user_seen_items = {}\n",
    "for u, m in zip(train_df[user_idx_col].astype(int).values,\n",
    "                train_df[movie_idx_col].astype(int).values):\n",
    "    user_seen_items.setdefault(int(u), set()).add(int(m))\n",
    "\n",
    "# ---- 4) Build id_to_title (TMDB id -> title) filtered to IDs present in embeddings ----\n",
    "# Try to auto-pick a movies dataframe with ['id','title'] columns\n",
    "movies_df = None\n",
    "for cand in (\"movies\", \"metadata_cleaned\", \"metadata\", \"movies_metadata\"):\n",
    "    if cand in globals():\n",
    "        df = globals()[cand]\n",
    "        if isinstance(df, pd.DataFrame) and {\"id\",\"title\"}.issubset(df.columns):\n",
    "            movies_df = df[[\"id\",\"title\"]].dropna()\n",
    "            break\n",
    "\n",
    "id_to_title = {}\n",
    "if movies_df is not None:\n",
    "    tmdb_array = np.asarray(tmdb_ids)  # must exist from Step 3/6\n",
    "    tmdb_set = set(int(x) for x in tmdb_array.tolist())\n",
    "    sub = movies_df[movies_df[\"id\"].astype(int).isin(tmdb_set)]\n",
    "    id_to_title = {int(r.id): str(r.title) for _, r in sub.iterrows()}\n",
    "\n",
    "# ---- 5) Write files (overwrites old ones) ----\n",
    "with open(\"./mappings/u2i.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({int(k): int(v) for k, v in u2i.items()}, f, ensure_ascii=False)\n",
    "\n",
    "with open(\"./mappings/user_seen_items.pkl\", \"wb\") as f:\n",
    "    pickle.dump({int(u): set(int(m) for m in s) for u, s in user_seen_items.items()}, f)\n",
    "\n",
    "if id_to_title:\n",
    "    with open(\"./mappings/id_to_title.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(id_to_title, f, ensure_ascii=False)\n",
    "\n",
    "print(f\"Wrote u2i.json ({len(u2i)} users), user_seen_items.pkl ({len(user_seen_items)} users), \"\n",
    "      f\"id_to_title.json ({len(id_to_title)} titles).\")\n",
    "print(\"Sample user IDs:\", list(sorted(u2i.keys()))[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
